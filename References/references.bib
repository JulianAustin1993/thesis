
@article{deb_spatio-temporal_2017,
	title = {Spatio-temporal models with space-time interaction and their applications to air pollution data},
	url = {http://arxiv.org/abs/1801.00211},
	abstract = {It is of utmost importance to have a clear understanding of the status of air pollution and to provide forecasts and insights about the air quality to the general public and researchers in environmental studies. Previous studies of spatio-temporal models showed that even a shortterm exposure to high concentrations of atmospheric ﬁne particulate matters can be hazardous to the health of ordinary people. In this study, we develop a spatio-temporal model with space-time interaction for air pollution data (PM2.5). The proposed model uses a parametric space-time interaction component along with the spatial and temporal components in the mean structure, and introduces a random-eﬀects component speciﬁed in the form of zeromean spatio-temporal processes. For application, we analyze the air pollution data (PM2.5) from 66 monitoring stations across Taiwan.},
	language = {en},
	urldate = {2018-06-22},
	journal = {arXiv:1801.00211 [stat]},
	author = {Deb, Soudeep and Tsay, Ruey S.},
	month = dec,
	year = {2017},
	note = {arXiv: 1801.00211},
	keywords = {Separable Covariance Models},
}

@book{cressie_statistics_2011,
	address = {Hoboken, N.J},
	series = {Wiley series in probability and statistics},
	title = {Statistics for spatio-temporal data},
	isbn = {978-0-471-69274-4},
	number = {1},
	publisher = {Wiley},
	author = {Cressie, Noel A. C. and Wikle, Christopher K.},
	year = {2011},
	keywords = {Simple Kriging, Ordinary Kriging, Universal Kriging},
}

@article{cressie_classes_1999,
	title = {Classes of {Nonseparable}, {Spatio}-temporal {Stationary} {Covariance} {Functions}},
	volume = {94},
	abstract = {this article, we derive a new approach that allows one to obtain many classes of nonseparable, spatio-temporal stationary covariance functions and we fit several such to spatio-temporal data on wind speed over a region in the tropical western Pacific ocean. 1. INTRODUCTION},
	journal = {Journal of the American Statistical Association},
	author = {Cressie, Noel and Huang, Hsin-cheng},
	year = {1999},
	keywords = {Non-Separable Covariance Models},
	pages = {1330--1340},
}

@article{iaco_nonseparable_2002,
	title = {Nonseparable {Space}-{Time} {Covariance} {Models}: {Some} {Parametric} {Families}},
	volume = {34},
	issn = {0882-8121, 1573-8868},
	shorttitle = {Nonseparable {Space}-{Time} {Covariance} {Models}},
	url = {https://link.springer.com/article/10.1023/A:1014075310344},
	doi = {10.1023/A:1014075310344},
	abstract = {By extending the product and product–sum space-time covariance models, new families are generated as integrated products and product–sums. These include nonintegrable space-time covariance models not obtainable by the Cressie–Huang representation. It is shown how to fit the spatial and temporal components of the models as well as the probability density function. The methods are illustrated by a case study.},
	language = {en},
	number = {1},
	urldate = {2018-06-22},
	journal = {Mathematical Geology},
	author = {Iaco, S. De and Myers, D. E. and Posa, D.},
	month = jan,
	year = {2002},
	keywords = {Non-Separable Covariance Models, Mixed Models},
	pages = {23--42},
}

@article{gneiting_nonseparable_2002,
	title = {Nonseparable, {Stationary} {Covariance} {Functions} for {Space}-{Time} {Data}},
	volume = {97},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/3085674},
	abstract = {Geostatistical approaches to spatiotemporal prediction in environmental science, climatology, meteorology, and related fields rely on appropriate covariance models. This article proposes general classes of nonseparable, stationary covariance functions for spatiotemporal random processes. The constructions are directly in the space-time domain and do not depend on closed-form Fourier inversions. The model parameters can be associated with the data's spatial and temporal structures, respectively; and a covariance model with a readily interpretable space-time interaction parameter is fitted to wind data from Ireland.},
	number = {458},
	urldate = {2018-06-27},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann},
	year = {2002},
	keywords = {Non-Separable Covariance Models},
	pages = {590--600},
}

@article{aston_tests_2017,
	title = {Tests for separability in nonparametric covariance operators of random surfaces},
	volume = {45},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1498636862},
	doi = {10.1214/16-AOS1495},
	abstract = {The assumption of separability of the covariance operator for a random image or hypersurface can be of substantial use in applications, especially in situations where the accurate estimation of the full covariance structure is unfeasible, either for computational reasons, or due to a small sample size. However, inferential tools to verify this assumption are somewhat lacking in high-dimensional or functional data analysis settings, where this assumption is most relevant. We propose here to test separability by focusing on KKK-dimensional projections of the difference between the covariance operator and a nonparametric separable approximation. The subspace we project onto is one generated by the eigenfunctions of the covariance operator estimated under the separability hypothesis, negating the need to ever estimate the full nonseparable covariance. We show that the rescaled difference of the sample covariance operator with its separable approximation is asymptotically Gaussian. As a by-product of this result, we derive asymptotically pivotal tests under Gaussian assumptions, and propose bootstrap methods for approximating the distribution of the test statistics. We probe the finite sample performance through simulations studies, and present an application to log-spectrogram images from a phonetic linguistics dataset.},
	language = {EN},
	number = {4},
	urldate = {2018-07-20},
	journal = {Ann. Statist.},
	author = {Aston, John A. D. and Pigoli, Davide and Tavakoli, Shahin},
	month = aug,
	year = {2017},
	mrnumber = {MR3670184},
	zmnumber = {06773279},
	pages = {1431--1461},
	file = {arXiv Fulltext PDF:/Users/b7064522/Zotero/storage/UE2KUHGT/Aston et al. - 2017 - Tests for separability in nonparametric covariance.pdf:application/pdf;arXiv.org Snapshot:/Users/b7064522/Zotero/storage/JCNHJMU4/1505.html:text/html;Full Text PDF:/Users/b7064522/Zotero/storage/LTL6DWMJ/Aston et al. - 2017 - Tests for separability in nonparametric covariance.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/NN3LT4NM/tests-for-separability-in-nonparametric-covariance-operators-of-random-surfaces(bf9cd29f-0336-4.html:text/html},
}

@book{ramsay_functional_2010,
	address = {New York (N.Y.)},
	title = {Functional data analysis},
	isbn = {978-1-4419-2300-4},
	language = {English},
	publisher = {Springer Science+Business Media},
	author = {Ramsay, James O and Silverman, Bernard W},
	year = {2010},
}

@article{mitchell_likelihood_2006,
	title = {A likelihood ratio test for separability of covariances},
	volume = {97},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X0500120X},
	doi = {10.1016/j.jmva.2005.07.005},
	abstract = {We propose a formal test of separability of covariance models based on a likelihood ratio statistic. The test is developed in the context of multivariate repeated measures (for example, several variables measured at multiple times on many subjects), but can also apply to a replicated spatio-temporal process and to problems in meteorology, where horizontal and vertical covariances are often assumed to be separable. Separable models are a common way to model spatio-temporal covariances because of the computational benefits resulting from the joint space–time covariance being factored into the product of a covariance function that depends only on space and a covariance function that depends only on time. We show that when the null hypothesis of separability holds, the distribution of the test statistic does not depend on the type of separable model. Thus, it is possible to develop reference distributions of the test statistic under the null hypothesis. These distributions are used to evaluate the power of the test for certain nonseparable models. The test does not require second-order stationarity, isotropy, or specification of a covariance model. We apply the test to a multivariate repeated measures problem.},
	number = {5},
	urldate = {2018-08-07},
	journal = {Journal of Multivariate Analysis},
	author = {Mitchell, Matthew W. and Genton, Marc G. and Gumpertz, Marcia L.},
	month = may,
	year = {2006},
	pages = {1025--1043},
}

@article{fuentes_testing_2006,
	title = {Testing for separability of spatial–temporal covariance functions},
	volume = {136},
	issn = {0378-3758},
	url = {http://www.sciencedirect.com/science/article/pii/S0378375804003210},
	doi = {10.1016/j.jspi.2004.07.004},
	abstract = {Most applications in spatial statistics involve modeling of complex spatial–temporal dependency structures, and many of the problems of space and time modeling can be overcome by using separable processes. This subclass of spatial–temporal processes has several advantages, including rapid fitting and simple extensions of many techniques developed and successfully used in time series and classical geostatistics. In particular, a major advantage of these processes is that the covariance matrix for a realization can be expressed as the Kronecker product of two smaller matrices that arise separately from the temporal and purely spatial processes, and hence its determinant and inverse are easily determinable. However, these separable models are not always realistic, and there are no formal tests for separability of general spatial–temporal processes. We present here a formal method to test for separability. Our approach can be also used to test for lack of stationarity of the process. The beauty of our approach is that by using spectral methods the mechanics of the test can be reduced to a simple two-factor analysis of variance (ANOVA) procedure. The approach we propose is based on only one realization of the spatial–temporal process. We apply the statistical methods proposed here to test for separability and stationarity of spatial–temporal ozone fields using data provided by the US Environmental Protection Agency (EPA).},
	number = {2},
	urldate = {2018-08-07},
	journal = {Journal of Statistical Planning and Inference},
	author = {Fuentes, Montserrat},
	month = feb,
	year = {2006},
	pages = {447--466},
}

@article{singha_satellite_2013,
	title = {Satellite {Oil} {Spill} {Detection} {Using} {Artificial} {Neural} {Networks}},
	volume = {6},
	issn = {1939-1404},
	doi = {10.1109/JSTARS.2013.2251864},
	abstract = {Oil spills represent a major threat to ocean ecosystems and their health. Illicit pollution requires continuous monitoring and satellite remote sensing technology represents an attractive option for operational oil spill detection. Previous studies have shown that active microwave satellite sensors, particularly Synthetic Aperture Radar (SAR) can be effectively used for the detection and classification of oil spills. Oil spills appear as dark spots in SAR images. However, similar dark spots may arise from a range of unrelated meteorological and oceanographic phenomena, resulting in misidentification. A major focus of research in this area is the development of algorithms to distinguish oil spills from `look-alikes'. This paper describes the development of a new approach to SAR oil spill detection employing two different Artificial Neural Networks (ANN), used in sequence. The first ANN segments a SAR image to identify pixels belonging to candidate oil spill features. A set of statistical feature parameters are then extracted and used to drive a second ANN which classifies objects into oil spills or look-alikes. The proposed algorithm was trained using 97 ERS-2 SAR and ENVSAT ASAR images of individual verified oil spills or/and look-alikes. The algorithm was validated using a large dataset comprising full-swath images and correctly identified 91.6\% of reported oil spills and 98.3\% of look-alike phenomena. The segmentation stage of the new technique outperformed the established edge detection and adaptive thresholding approaches. An analysis of feature descriptors highlighted the importance of image gradient information in the classification stage.},
	number = {6},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Singha, S. and Bellerby, T. J. and Trieschmann, O.},
	month = dec,
	year = {2013},
	pages = {2355--2363},
}

@book{oliver_understanding_2004,
	address = {Raleigh, NC},
	title = {Understanding {Synthetic} {Aperture} {Radar} {Images}},
	isbn = {978-1-891121-31-9},
	abstract = {Written for SAR system designers and remote sensing specialists, this practical reference shows you how to produce higher quality SAR images using data-driven algorithms, and how to apply powerful new techniques to measure and analyze SAR image content.The book describes how SAR imagery is formed, how SAR processing affects image properties, and gives you specific guidance in selecting and applying today's most sophisticated analytical techniques. By helping you to quickly assess which components of an SAR image should be measured, the book enables you to devote more time and energy to the real task of image interpretation and analysis.Now includes a CD-ROM featuring a 2-month free license to InfoPACK Version 1.2! InfoPACK, developed by Chris Oliver, is a SAR image interpretation software suite which exploits and extends the principles described in the book, enabling the user to run many of the algorithms. It features an easy to use GUI, image viewer and versatile scripting language, making applications development easy and fast. Particular aspects of interest include segmentation, classification (supervised and unsupervised), speckle reduction and a host of techniques for data fusion. Routines within InfoPACK include:* Intensity Segmentation (single image; multitemporal, multipolarization)* Speckle Reduction* Texture Segmentation* Segmentation Postprocessing* Classification* Edge Detection * Point Target Detection* Large Area Change DetectionAlso included are a variety of low-level filters and functions for image manipulation and arithmetic operations.Key Features of the Book* Filled with real-world examples from various SAR systems, the book reveals sophisticated, proven techniques for:* SAR filtering, parameter estimation, image reconstruction, segmentation, and classification * Evaluating measurement algorithms to help make better algorithm selections * Applying powerful speckle removal methods to airborne and spaceborne SAR data},
	language = {English},
	publisher = {Scitech Publishing},
	author = {Oliver, Christopher},
	month = dec,
	year = {2004},
}

@incollection{militino_introduction_2018,
	address = {Cham},
	title = {An {Introduction} to the {Spatio}-{Temporal} {Analysis} of {Satellite} {Remote} {Sensing} {Data} for {Geostatisticians}},
	isbn = {978-3-319-78999-6},
	url = {https://doi.org/10.1007/978-3-319-78999-6_13},
	abstract = {Satellite remote sensing data have become available in meteorology, agriculture, forestry, geology, regional planning, hydrology or natural environment sciences since several decades ago, because satellites provide routinely high quality images with different temporal and spatial resolutions. Joining, combining or smoothing these images for a better quality of information is a challenge not always properly solved. In this regard, geostatistics, as the spatio-temporal stochastic techniques of geo-referenced data, is a very helpful and powerful tool not enough explored in this area yet. Here, we analyze the current use of some of the geostatistical tools in satellite image analysis, and provide an introduction to this subject for potential researchers.},
	language = {en},
	urldate = {2019-06-02},
	booktitle = {Handbook of {Mathematical} {Geosciences}: {Fifty} {Years} of {IAMG}},
	publisher = {Springer International Publishing},
	author = {Militino, A. F. and Ugarte, M. D. and Pérez-Goya, U.},
	editor = {Daya Sagar, B.S. and Cheng, Qiuming and Agterberg, Frits},
	year = {2018},
	doi = {10.1007/978-3-319-78999-6_13},
	pages = {239--253},
	file = {Springer Full Text PDF:/Users/b7064522/Zotero/storage/SCY6D7AG/Militino et al. - 2018 - An Introduction to the Spatio-Temporal Analysis of.pdf:application/pdf},
}

@article{aschbacher_european_2012,
	series = {The {Sentinel} {Missions} - {New} {Opportunities} for {Science}},
	title = {The {European} {Earth} monitoring ({GMES}) programme: {Status} and perspectives},
	volume = {120},
	issn = {0034-4257},
	shorttitle = {The {European} {Earth} monitoring ({GMES}) programme},
	url = {http://www.sciencedirect.com/science/article/pii/S0034425712000612},
	doi = {10.1016/j.rse.2011.08.028},
	abstract = {Global Monitoring for Environment and Security (GMES) is the most ambitious operational Earth Observation programme to date and will provide global, timely and easily accessible information in application domains such as land, marine, atmosphere, emergency response, climate change and security. To accomplish this, the European Union (EU)-led GMES programme comprises three components namely the space component, the in-situ component and the service component. The space component, led by ESA, is in its pre-operational stage, serving users with satellite data acquired by the so called “GMES Contributing Missions” already available today or planned at European, national and international level. It will become operational once the dedicated space infrastructure, comprised by the “Sentinel” missions and their corresponding ground segments are operational. The first of these satellite series will be launched in 2013. The Sentinel missions will provide a unique set of observations utilising different techniques spanning C-band SAR, mid to medium resolution optical and thermal observations with increased spectral resolution, altimeter and dedicated spectrometers for atmospheric chemistry. This data, combined with in-situ data, some assimilated into models, will then be turned into services for monitoring the environment, the climate and for security related issues. The GMES Space Component (GSC) is organised in two overlapping phases: the development phase and the operational phase, the latter planned to start in 2014. The main challenge is now to ensure the programme's long-term sustainability.},
	urldate = {2019-06-17},
	journal = {Remote Sensing of Environment},
	author = {Aschbacher, Josef and Milagro-Pérez, Maria Pilar},
	month = may,
	year = {2012},
	pages = {3--8},
	file = {ScienceDirect Full Text PDF:/Users/b7064522/Zotero/storage/SMFLBCG2/Aschbacher and Milagro-Pérez - 2012 - The European Earth monitoring (GMES) programme St.pdf:application/pdf;ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/IVXY3JQM/S0034425712000612.html:text/html},
}

@article{george_selecting_2015,
	title = {Selecting a {Separable} {Parametric} {Spatiotemporal} {Covariance} {Structure} for {Longitudinal} {Imaging} {Data}},
	volume = {34},
	issn = {0277-6715},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4262538/},
	doi = {10.1002/sim.6324},
	abstract = {Longitudinal imaging studies allow great insight into how the structure and function of a subject’s internal anatomy changes over time. Unfortunately, the analysis of longitudinal imaging data is complicated by inherent spatial and temporal correlation: the temporal from the repeated measures, and the spatial from the outcomes of interest being observed at multiple points in a patients body. We propose the use of a linear model with a separable parametric spatiotemporal error structure for the analysis of repeated imaging data. The model makes use of spatial (exponential, spherical, and Matérn) and temporal (compound symmetric, autoregressive-1, Toeplitz, and unstructured) parametric correlation functions., A simulation study, inspired by a longitudinal cardiac imaging study on mitral regurgitation patients, compared different information criteria for selecting a particular separable parametric spatiotemporal correlation structure as well as the effects on Type I and II error rates for inference on fixed effects when the specified model is incorrect. Information criteria were found to be highly accurate at choosing between separable parametric spatiotemporal correlation structures. Misspecification of the covariance structure was found to have the ability to inflate the Type I error or have an overly conservative test size, which corresponded to decreased power., An example with clinical data is given illustrating how the covariance structure procedure can be done in practice, as well as how covariance structure choice can change inferences about fixed effects.},
	number = {1},
	urldate = {2019-06-24},
	journal = {Stat Med},
	author = {George, Brandon and Aban, Inmaculada},
	month = jan,
	year = {2015},
	pmid = {25293361},
	pmcid = {PMC4262538},
	pages = {145--161},
	file = {PubMed Central Full Text PDF:/Users/b7064522/Zotero/storage/HXL239HQ/George and Aban - 2015 - Selecting a Separable Parametric Spatiotemporal Co.pdf:application/pdf},
}

@book{shi_gaussian_2011,
	title = {Gaussian {Process} {Regression} {Analysis} for {Functional} {Data}},
	isbn = {978-1-4398-3773-3},
	abstract = {Gaussian Process Regression Analysis for Functional Data presents nonparametric statistical methods for functional regression analysis, specifically the methods based on a Gaussian process prior in a functional space. The authors focus on problems involving functional response variables and mixed covariates of functional and scalar variables.  Covering the basics of Gaussian process regression, the first several chapters discuss functional data analysis, theoretical aspects based on the asymptotic properties of Gaussian process regression models, and new methodological developments for high dimensional data and variable selection. The remainder of the text explores advanced topics of functional regression analysis, including novel nonparametric statistical methods for curve prediction, curve clustering, functional ANOVA, and functional regression analysis of batch data, repeated curves, and non-Gaussian data.  Many flexible models based on Gaussian processes provide efficient ways of model learning, interpreting model structure, and carrying out inference, particularly when dealing with large dimensional functional data. This book shows how to use these Gaussian process regression models in the analysis of functional data. Some MATLAB® and C codes are available on the first author’s website.},
	language = {en},
	publisher = {CRC Press},
	author = {Shi, Jian Qing and Choi, Taeryon},
	month = jul,
	year = {2011},
	note = {Google-Books-ID: DkgdN6dRAicC},
}

@article{wood_p-splines_2017,
	title = {P-splines with derivative based penalties and tensor product smoothing of unevenly distributed data},
	volume = {27},
	issn = {0960-3174, 1573-1375},
	url = {http://arxiv.org/abs/1605.02446},
	doi = {10.1007/s11222-016-9666-x},
	abstract = {The P-splines of Eilers and Marx (1996) combine a B-spline basis with a discrete quadratic penalty on the basis coefﬁcients, to produce a reduced rank spline like smoother. P-splines have three properties that make them very popular as reduced rank smoothers: i) the basis and the penalty are sparse, enabling efﬁcient computation, especially for Bayesian stochastic simulation; ii) it is possible to ﬂexibly ‘mix-and-match’ the order of B-spline basis and penalty, rather than the order of penalty controlling the order of the basis as in spline smoothing; iii) it is very easy to set up the Bspline basis functions and penalties. The discrete penalties are somewhat less interpretable in terms of function shape than the traditional derivative based spline penalties, but tend towards penalties proportional to traditional spline penalties in the limit of large basis size. However part of the point of P-splines is not to use a large basis size. In addition the spline basis functions arise from solving functional optimization problems involving derivative based penalties, so moving to discrete penalties for smoothing may not always be desirable. The purpose of this note is to point out that the three properties of basis-penalty sparsity, mix-and-match penalization and ease of setup are readily obtainable with B-splines subject to derivative based penalization. The penalty setup typically requires a few lines of code, rather than the two lines typically required for P-splines, but this one off disadvantage seems to be the only one associated with using derivative based penalties. As an example application, it is shown how basis-penalty sparsity enables efﬁcient computation with tensor product smoothers of scattered data.},
	language = {en},
	number = {4},
	urldate = {2020-03-11},
	journal = {Stat Comput},
	author = {Wood, Simon N.},
	month = jul,
	year = {2017},
	note = {arXiv: 1605.02446},
	pages = {985--989},
}

@book{wahba_spline_1990,
	series = {{CBMS}-{NSF} {Regional} {Conference} {Series} in {Applied} {Mathematics}},
	title = {Spline {Models} for {Observational} {Data}},
	isbn = {978-0-89871-244-5},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611970128},
	abstract = {This monograph is based on a series of 10 lectures at Ohio State University at Columbus, March 23–27, 1987, sponsored by the Conference Board of the Mathematical Sciences and the National Science Foundation. The selection of topics is quite personal and, together with the talks of the other speakers, the lectures represent a story, as I saw it in March 1987, of many of the interesting things that statisticians can do with splines. I told the audience that the priority order for topic selection was, first, obscure work of my own and collaborators, second, other work by myself and students, with important work by other speakers deliberately omitted in the hope that they would mention it themselves. This monograph will more or less follow that outline, so that it is very much slanted toward work I had some hand in, although I will try to mention at least by reference important work by the other speakers and some of the attendees. The other speakers were (in alphabetical order), Dennis Cox, Randy Eubank, Ker-Chau Li, Douglas Nychka, David Scott, Bernard Silverman, Paul Speckman, and James Wendelberger. The work of Finbarr O'Sullivan, who was unable to attend, in extending the developing theory to the non-Gaussian and nonlinear case will also play a central role, as will the work of Florencio Utreras.},
	urldate = {2020-04-24},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Wahba, Grace},
	month = jan,
	year = {1990},
	doi = {10.1137/1.9781611970128},
	file = {Snapshot:/Users/b7064522/Zotero/storage/C8BVU8RL/1.html:text/html},
}

@book{hyndman_forecasting_2018,
	title = {Forecasting: principles and practice},
	isbn = {978-0-9875071-1-2},
	shorttitle = {Forecasting},
	abstract = {Forecasting is required in many situations. Stocking an inventory may require forecasts of demand months in advance. Telecommunication routing requires traffic forecasts a few minutes ahead. Whatever the circumstances or time horizons involved, forecasting is an important aid in effective and efficient planning.This textbook provides a comprehensive introduction to forecasting methods and presents enough information about each method for readers to use them sensibly.},
	language = {en},
	publisher = {OTexts},
	author = {Hyndman, Rob J. and Athanasopoulos, George},
	month = may,
	year = {2018},
	note = {Google-Books-ID: \_bBhDwAAQBAJ},
}

@article{muro_short-term_2016,
	title = {Short-{Term} {Change} {Detection} in {Wetlands} {Using} {Sentinel}-1 {Time} {Series}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/8/10/795},
	doi = {10.3390/rs8100795},
	abstract = {Automated monitoring systems that can capture wetlands’ high spatial and temporal variability are essential for their management. SAR-based change detection approaches offer a great opportunity to enhance our understanding of complex and dynamic ecosystems. We test a recently-developed time series change detection approach (S1-omnibus) using Sentinel-1 imagery of two wetlands with different ecological characteristics; a seasonal isolated wetland in southern Spain and a coastal wetland in the south of France. We test the S1-omnibus method against a commonly-used pairwise comparison of consecutive images to demonstrate its advantages. Additionally, we compare it with a pairwise change detection method using a subset of consecutive Landsat images for the same period of time. The results show how S1-omnibus is capable of capturing in space and time changes produced by water surface dynamics, as well as by agricultural practices, whether they are sudden changes, as well as gradual. S1-omnibus is capable of detecting a wider array of short-term changes than when using consecutive pairs of Sentinel-1 images. When compared to the Landsat-based change detection method, both show an overall good agreement, although certain landscape changes are detected only by either the Landsat-based or the S1-omnibus method. The S1-omnibus method shows a great potential for an automated monitoring of short time changes and accurate delineation of areas of high variability and of slow and gradual changes.},
	language = {en},
	number = {10},
	urldate = {2020-04-24},
	journal = {Remote Sensing},
	author = {Muro, Javier and Canty, Morton and Conradsen, Knut and Hüttich, Christian and Nielsen, Allan Aasbjerg and Skriver, Henning and Remy, Florian and Strauch, Adrian and Thonfeld, Frank and Menz, Gunter},
	month = oct,
	year = {2016},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {795},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/3LH6YJTT/Muro et al. - 2016 - Short-Term Change Detection in Wetlands Using Sent.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/WIIX4U8D/795.html:text/html},
}

@inproceedings{hooker_maximal_2015,
	title = {Maximal autocorrelation factors for function-valued spatial/temporal data},
	isbn = {978-0-9872143-5-5},
	url = {http://www.mssanz.org.au/modsim2015/A3/hooker.pdf},
	doi = {10.36334/MODSIM.2015.A3.Hooker},
	abstract = {Dimension reduction techniques play a key role in analyzing functional data that possess temporal or spatial dependence. Of these dimension reduction techniques functional principal components analysis (FPCA) remains a popular approach. Functional principal components extract a set of latent components by maximizing variance in a set of dependent functional data. However, this technique may fail to adequately capture temporal or spatial autocorrelation.},
	language = {en},
	urldate = {2020-05-01},
	booktitle = {Weber, {T}., {McPhee}, {M}.{J}. and {Anderssen}, {R}.{S}. (eds) {MODSIM2015}, 21st {International} {Congress} on {Modelling} and {Simulation}},
	publisher = {Modelling and Simulation Society of Australia and New Zealand},
	author = {Hooker, G and Roberts, S and Shang, Lin, Han},
	month = nov,
	year = {2015},
	file = {2015 - Maximal autocorrelation factors for function-value.pdf:/Users/b7064522/Zotero/storage/HYK7V3CD/2015 - Maximal autocorrelation factors for function-value.pdf:application/pdf},
}

@article{hooker_maximal_2015-1,
	title = {Maximal autocorrelation factors for function-valued spatial/temporal data},
	author = {Hooker, Giles and Roberts, Steven and Shang, Hanlin and {others}},
	year = {2015},
	note = {Publisher: The Modelling and Simulation Society of Australia and New Zealand Inc.},
}

@article{karhunen_zur_1946,
	title = {Zur {Spektraltheorie} stochastischer {Prozesse}},
	volume = {1},
	journal = {Ann. Acad. Sci. Finnicae, Ser. A},
	author = {Karhunen, K.},
	year = {1946},
	keywords = {imported},
	pages = {34},
}

@article{loeve_fonctions_1946,
	title = {Fonctions aléatoires à décomposition orthogonale exponentielle},
	volume = {84},
	journal = {La Revue Scientifique},
	author = {Loève, Michel},
	year = {1946},
	pages = {159--162},
}

@article{switzer_minmax_1984,
	title = {Min/{Max} autocorrelation factors for multivariate spatial imaging},
	journal = {Technical Report No.6},
	author = {Switzer, P. and Green, A.A.},
	year = {1984},
	note = {Publisher: Department of Statistics, Stanford University, Stanford, CA},
	pages = {14},
}

@article{hyndman_automatic_2008,
	title = {Automatic time series forecasting: the forecast package for {R}},
	volume = {26},
	url = {http://www.jstatsoft.org/article/view/v027i03},
	number = {3},
	journal = {Journal of Statistical Software},
	author = {Hyndman, Rob J. and Khandakar, Yeasmin},
	year = {2008},
	pages = {1--22},
}

@book{de_boor_practical_2001,
	address = {New York},
	edition = {Rev. ed},
	series = {Applied mathematical sciences},
	title = {A practical guide to splines: with 32 figures},
	isbn = {978-0-387-95366-3},
	shorttitle = {A practical guide to splines},
	number = {v. 27},
	publisher = {Springer},
	author = {De Boor, Carl},
	year = {2001},
	file = {Snapshot:/Users/b7064522/Zotero/storage/M4AQHHVZ/Boor - 1978 - A Practical Guide to Splines.html:text/html;Snapshot:/Users/b7064522/Zotero/storage/XZK2GBMT/9780387953663.html:text/html},
}

@article{wold_principal_1987,
	title = {Principal component analysis},
	volume = {2},
	number = {1-3},
	journal = {Chemometrics and intelligent laboratory systems},
	author = {Wold, Svante and Esbensen, Kim and Geladi, Paul},
	year = {1987},
	note = {Publisher: Elsevier},
	pages = {37--52},
}

@article{yao_functional_2005,
	title = {Functional {Data} {Analysis} for {Sparse} {Longitudinal} {Data}},
	volume = {100},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/27590579},
	abstract = {We propose a nonparametric method to perform functional principal components analysis for the case of sparse longitudinal data. The method aims at irregularly spaced longitudinal data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject. We assume that the repeated measurements are located randomly with a random number of repetitions for each subject and are determined by an underlying smooth random (subject-specific) trajectory plus measurement errors. Basic elements of our approach are the parsimonious estimation of the covariance structure and mean function of the trajectories, and the estimation of the variance of the measurement errors. The eigenfunction basis is estimated from the data, and functional principal components score estimates are obtained by a conditioning step. This conditional estimation method is conceptually simple and straightforward to implement. A key step is the derivation of asymptotic consistency and distribution results under mild conditions, using tools from functional analysis. Functional data analysis for sparse longitudinal data enables prediction of individual smooth trajectories even if only one or few measurements are available for a subject. Asymptotic pointwise and simultaneous confidence bands are obtained for predicted individual trajectories, based on asymptotic distributions, for simultaneous bands under the assumption of a finite number of components. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. The methods are illustrated with a simulation study, longitudinal CD4 data for a sample of AIDS patients, and time-course gene expression data for the yeast cell cycle.},
	number = {470},
	urldate = {2020-05-14},
	journal = {Journal of the American Statistical Association},
	author = {Yao, Fang and Müller, Hans-Georg and Wang, Jane-Ling},
	year = {2005},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {577--590},
	file = {Yao et al. - 2005 - Functional Data Analysis for Sparse Longitudinal D.pdf:/Users/b7064522/Zotero/storage/SW96SQ9Y/Yao et al. - 2005 - Functional Data Analysis for Sparse Longitudinal D.pdf:application/pdf},
}

@book{williams_gaussian_2006,
	title = {Gaussian processes for machine learning},
	volume = {2},
	number = {3},
	publisher = {MIT press Cambridge, MA},
	author = {Williams, Christopher KI and Rasmussen, Carl Edward},
	year = {2006},
}

@article{hooker_maximal_2016,
	title = {Maximal autocorrelation functions in functional data analysis},
	volume = {26},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-015-9582-5},
	doi = {10.1007/s11222-015-9582-5},
	abstract = {This paper proposes a new factor rotation for the context of functional principal components analysis. This rotation seeks to re-express a functional subspace in terms of directions of decreasing smoothness as represented by a generalized smoothing metric. The rotation can be implemented simply and we show on two examples that this rotation can improve the interpretability of the leading components.},
	language = {en},
	number = {5},
	urldate = {2020-06-04},
	journal = {Stat Comput},
	author = {Hooker, Giles and Roberts, Steven},
	month = sep,
	year = {2016},
	pages = {945--950},
}

@article{liu_functional_2017,
	title = {Functional principal component analysis of spatially correlated data},
	volume = {27},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-016-9708-4},
	doi = {10.1007/s11222-016-9708-4},
	abstract = {This paper focuses on the analysis of spatially correlated functional data. We propose a parametric model for spatial correlation and the between-curve correlation is modeled by correlating functional principal component scores of the functional data. Additionally, in the sparse observation framework, we propose a novel approach of spatial principal analysis by conditional expectation to explicitly estimate spatial correlations and reconstruct individual curves. Assuming spatial stationarity, empirical spatial correlations are calculated as the ratio of eigenvalues of the smoothed covariance surface Cov\$\$(X\_i(s),X\_i(t))\$\$(Xi(s),Xi(t))and cross-covariance surface Cov\$\$(X\_i(s), X\_j(t))\$\$(Xi(s),Xj(t))at locations indexed by i and j. Then a anisotropy Matérn spatial correlation model is fitted to empirical correlations. Finally, principal component scores are estimated to reconstruct the sparsely observed curves. This framework can naturally accommodate arbitrary covariance structures, but there is an enormous reduction in computation if one can assume the separability of temporal and spatial components. We demonstrate the consistency of our estimates and propose hypothesis tests to examine the separability as well as the isotropy effect of spatial correlation. Using simulation studies, we show that these methods have some clear advantages over existing methods of curve reconstruction and estimation of model parameters.},
	language = {en},
	number = {6},
	urldate = {2020-06-19},
	journal = {Stat Comput},
	author = {Liu, Chong and Ray, Surajit and Hooker, Giles},
	month = nov,
	year = {2017},
	pages = {1639--1654},
	file = {Liu et al. - 2017 - Functional principal component analysis of spatial.pdf:/Users/b7064522/Zotero/storage/IBUFMS9S/Liu et al. - 2017 - Functional principal component analysis of spatial.pdf:application/pdf;Springer Full Text PDF:/Users/b7064522/Zotero/storage/T3VPCMSN/Liu et al. - 2017 - Functional principal component analysis of spatial.pdf:application/pdf},
}

@article{liu_functional_2012,
	title = {Functional factor analysis for periodic remote sensing data},
	volume = {6},
	issn = {1932-6157},
	url = {http://projecteuclid.org/euclid.aoas/1339419609},
	doi = {10.1214/11-AOAS518},
	language = {en},
	number = {2},
	urldate = {2020-07-09},
	journal = {Ann. Appl. Stat.},
	author = {Liu, Chong and Ray, Surajit and Hooker, Giles and Friedl, Mark},
	month = jun,
	year = {2012},
	pages = {601--624},
	file = {Liu et al. - 2012 - Functional factor analysis for periodic remote sen.pdf:/Users/b7064522/Zotero/storage/GJUIBF4H/Liu et al. - 2012 - Functional factor analysis for periodic remote sen.pdf:application/pdf},
}

@article{khabbazan_crop_2019,
	title = {Crop {Monitoring} {Using} {Sentinel}-1 {Data}: {A} {Case} {Study} from {The} {Netherlands}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Crop {Monitoring} {Using} {Sentinel}-1 {Data}},
	url = {https://www.mdpi.com/2072-4292/11/16/1887},
	doi = {10.3390/rs11161887},
	abstract = {Agriculture is of huge economic significance in The Netherlands where the provision of real-time, reliable information on crop development is essential to support the transition towards precision agriculture. Optical imagery can provide invaluable insights into crop growth and development but is severely hampered by cloud cover. This case study in the Flevopolder illustrates the potential value of Sentinel-1 for monitoring five key crops in The Netherlands, namely sugar beet, potato, maize, wheat and English rye grass. Time series of radar backscatter from the European Space Agency’s Sentinel-1 Mission are analyzed and compared to ground measurements including phenological stage and height. Temporal variations in backscatter data reflect changes in water content and structure associated with phenological development. Emergence and closure dates are estimated from the backscatter time series and validated against a photo archive. Coherence data are compared to Normalized Difference Vegetation Index (NDVI) and ground data, illustrating that the sudden increase in coherence is a useful indicator of harvest. The results presented here demonstrate that Sentinel-1 data have significant potential value to monitor growth and development of key Dutch crops. Furthermore, the guaranteed availability of Sentinel-1 imagery in clouded conditions ensures the reliability of data to meet the monitoring needs of farmers, food producers and regulatory bodies.},
	language = {en},
	number = {16},
	urldate = {2020-11-04},
	journal = {Remote Sensing},
	author = {Khabbazan, Saeed and Vermunt, Paul and Steele-Dunne, Susan and Ratering Arntz, Lexy and Marinetti, Caterina and van der Valk, Dirk and Iannini, Lorenzo and Molijn, Ramses and Westerdijk, Kees and van der Sande, Corné},
	month = jan,
	year = {2019},
	note = {Number: 16
Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {1887},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/MR8ZQ8MT/Khabbazan et al. - 2019 - Crop Monitoring Using Sentinel-1 Data A Case Stud.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/FZKW24Y2/1887.html:text/html;Snapshot:/Users/b7064522/Zotero/storage/LRBI57WR/1887.html:text/html},
}

@article{raspini_continuous_2018,
	title = {Continuous, semi-automatic monitoring of ground deformation using {Sentinel}-1 satellites},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-25369-w},
	doi = {10.1038/s41598-018-25369-w},
	abstract = {We present the continuous monitoring of ground deformation at regional scale using ESA (European Space Agency) Sentinel-1constellation of satellites. We discuss this operational monitoring service through the case study of the Tuscany Region (Central Italy), selected due to its peculiar geological setting prone to ground instability phenomena. We set up a systematic processing chain of Sentinel-1 acquisitions to create continuously updated ground deformation data to mark the transition from static satellite analysis, based on the analysis of archive images, to dynamic monitoring of ground displacement. Displacement time series, systematically updated with the most recent available Sentinel-1 acquisition, are analysed to identify anomalous points (i.e., points where a change in the dynamic of motion is occurring). The presence of a cluster of persistent anomalies affecting elements at risk determines a significant level of risk, with the necessity of further analysis. Here, we show that the Sentinel-1 constellation can be used for continuous and systematic tracking of ground deformation phenomena at the regional scale. Our results demonstrate how satellite data, acquired with short revisiting times and promptly processed, can contribute to the detection of changes in ground deformation patterns and can act as a key information layer for risk mitigation.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Scientific Reports},
	author = {Raspini, Federico and Bianchini, Silvia and Ciampalini, Andrea and Del Soldato, Matteo and Solari, Lorenzo and Novali, Fabrizio and Del Conte, Sara and Rucci, Alessio and Ferretti, Alessandro and Casagli, Nicola},
	month = may,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {7253},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/P36EWZ9E/Raspini et al. - 2018 - Continuous, semi-automatic monitoring of ground de.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/WCPPKAQM/s41598-018-25369-w.html:text/html},
}

@book{cressie_statistics_2010,
	address = {New York},
	title = {Statistics for spatial data},
	isbn = {978-1-119-11515-1 978-0-471-00255-0},
	abstract = {The Wiley Classics Library consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists. Spatial statistics - analyzing spatial data through statistical models - has proven exceptionally versatile, encompassing problems ranging from the microscopic to the astronomic. However, for the scientist and engineer faced only with scattered and uneven treatments of the subject in the scientific literature, learning how to make practical use of spatial statistics in day-to-day analytical work is very difficult. Designed exclusively for scientists eager to tap into the enormous potential of this analytical tool and upgrade their range of technical skills, Statistics for Spatial Data is a comprehensive, single-source guide to both the theory and applied aspects of spatial statistical methods. The hard-cover edition was hailed by Mathematical Reviews as an "excellent book which will become a basic reference." This paper-back edition of the 1993 edition, is designed to meet the many technological challenges facing the scientist and engineer. Concentrating on the three areas of geostatistical data, lattice data, and point patterns, the book sheds light on the link between data and model, revealing how design, inference, and diagnostics are an outgrowth of that link. It then explores new methods to reveal just how spatial statistical models can be used to solve important problems in a host of areas in science and engineering. Discussion includes: exploratory spatial data analysis, spectral theory for stationary processes, spatial scale, simulation methods for spatial processes, spatial bootstrapping, statistical image analysis and remote sensing, computational aspects of model fitting, application of models to disease mapping. Designed to accommodate the practical needs of the professional, it features a unified and common notation for its subject as well as many detailed examples woven into the text, numerous illustrations (including graphs that illuminate the theory discussed) and over 1,000 references. Fully balancing theory with applications, Statistics for Spatial Data, Revised Edition is an exceptionally clear guide on making optimal use of one of the ascendant analytical tools of the decade, one that has begun to capture the imagination of professionals in biology, earth science, civil, electrical, and agricultural engineering, geography, epidemiology, and ecology.},
	language = {English},
	publisher = {Wiley},
	author = {Cressie, Noel A. C},
	year = {2010},
	note = {OCLC: 1039155476},
}

@book{stein_interpolation_1999,
	title = {Interpolation of {Spatial} {Data} {Some} {Theory} for {Kriging}},
	isbn = {978-1-4612-1494-6},
	url = {https://doi.org/10.1007/978-1-4612-1494-6},
	language = {German},
	urldate = {2020-11-04},
	author = {Stein, Michael L},
	year = {1999},
	note = {OCLC: 1184292315},
}

@article{rossi_kriging_1994,
	title = {Kriging in the shadows: {Geostatistical} interpolation for remote sensing},
	volume = {49},
	issn = {0034-4257},
	shorttitle = {Kriging in the shadows},
	url = {http://www.sciencedirect.com/science/article/pii/0034425794900574},
	doi = {10.1016/0034-4257(94)90057-4},
	abstract = {It is often useful to estimate obscured or missing remotely sensed data. Traditional interpolation methods, such as nearest-neighbor or bilinear resampling, do not take full advantage of the spatial information in the image. An alternative method, a geostatistical technique known as indicator kriging, is described and demonstrated using a Landsat Thematic Mapper image in southern Chiapas, Mexico. The image was first classified into pasture and nonpasture land cover. For each pixel that was obscured by cloud or cloud shadow, the probability that it was pasture was assigned by the algorithm. An exponential omnidirectional variogram model was used to characterize the spatial continuity of the image for use in the kriging algorithm. Assuming a cutoff probability level of 50\%, the error was shown to be 17\% with no obvious spatial bias but with some tendency to categorize nonpasture as pasture (overestimation). While this is a promising result, the method's practical application in other missing data problems for remotely sensed images will depend on the amount and spatial pattern of the unobscured pixels and missing pixels and the success of the spatial continuity model used.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Remote Sensing of Environment},
	author = {Rossi, Richard E. and Dungan, Jennifer L. and Beck, Louisa R.},
	month = jul,
	year = {1994},
	pages = {32--40},
	file = {ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/CXVK2VRI/0034425794900574.html:text/html},
}

@article{zhang_restoration_2009,
	title = {Restoration of clouded pixels in multispectral remotely sensed imagery with cokriging},
	volume = {30},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431160802549294},
	doi = {10.1080/01431160802549294},
	abstract = {The presence of clouds and their shadows in remotely sensed images limits their potential uses for extracting information. The commonly used methods for replacing clouded pixels by land cover reflection estimates usually yield poor results if the images being combined exhibit radical differences in target radiance due, for example, to large date separation and high temporal variability. This study focuses on introducing geostatistical techniques for interpolating the DN values of clouded pixels in multispectral remotely sensed images using traditional ordinary cokriging and standardized ordinary cokriging. Two case studies were conducted in this study. The first case study shows that the methods work well for the small clouds in a heterogeneous landscape even when the images being combined show high temporal variability. Although the basic spatial structure in large size clouds can be captured, image interpolation‐related artefacts such as smoothing effects are visually apparent in a heterogeneous landscape. The second case study indicates that the cokriging methods work better in homogenous regions such as the dominantly agricultural areas in United States Midwest. Various statistics including both global statistics and local statistics are employed to confirm the reliability of the methods.},
	number = {9},
	urldate = {2020-11-04},
	journal = {International Journal of Remote Sensing},
	author = {Zhang, Chuanrong and Li, Weidong and Travis, David J.},
	month = may,
	year = {2009},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01431160802549294},
	pages = {2173--2195},
	file = {Snapshot:/Users/b7064522/Zotero/storage/K3KVL58K/01431160802549294.html:text/html},
}

@article{schmidt_flexible_2020,
	series = {Frontiers in {Spatial} and {Spatio}-temporal {Research}},
	title = {Flexible spatial covariance functions},
	volume = {37},
	issn = {2211-6753},
	url = {http://www.sciencedirect.com/science/article/pii/S2211675320300105},
	doi = {10.1016/j.spasta.2020.100416},
	abstract = {We focus on the discussion of modeling processes that are observed at fixed locations of a region (geostatistics). A standard approach is to assume that the process of interest follows a Gaussian Process with some mean and (valid) covariance functions. It is common to model the covariance function as the product between a variance parameter, and a correlation function which is a function of the Euclidean distance between locations. This implies that the distribution of the process is unchanged when the origin of the index set is translated, and the process is invariant under rotation about the origin; that is the process is stationary and isotropic or homogeneous. However, the assumption of stationarity and isotropy (homogeneity) rarely holds in practice. Commonly, the correlation structures of such processes are influenced by local characteristics resulting in different behaviors in neighborhoods of different spatial locations. We review models that allow for heterogeneous covariance structures and point to some avenues of future research.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Spatial Statistics},
	author = {Schmidt, Alexandra M. and Guttorp, Peter},
	month = jun,
	year = {2020},
	pages = {100416},
	file = {ScienceDirect Full Text PDF:/Users/b7064522/Zotero/storage/D5EV3X6Q/Schmidt and Guttorp - 2020 - Flexible spatial covariance functions.pdf:application/pdf;ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/SPHNF6Z2/S2211675320300105.html:text/html},
}

@book{ferraty_nonparametric_2006,
	address = {New York},
	title = {Nonparametric {Functional} {Data} {Analysis}},
	isbn = {978-0-387-30369-7 978-0-387-36620-3},
	language = {English},
	publisher = {Springer Science+Business Media, Inc.},
	author = {Ferraty, Frédéric and Vieu, Philippe},
	year = {2006},
	note = {OCLC: 318296135},
}

@book{knott_interpolating_2000,
	series = {Progress in {Computer} {Science} and {Applied} {Logic}},
	title = {Interpolating {Cubic} {Splines}},
	isbn = {978-1-4612-7092-8},
	url = {https://www.springer.com/gp/book/9781461270928},
	abstract = {A spline is a thin flexible strip composed of a material such as bamboo or steel that can be bent to pass through or near given points in the plane, or in 3-space in a smooth manner. Mechanical engineers and drafting specialists find such (physical) splines useful in designing and in drawing plans for a wide variety of objects, such as for hulls of boats or for the bodies of automobiles where smooth curves need to be specified. These days, physi­ cal splines are largely replaced by computer software that can compute the desired curves (with appropriate encouragment). The same mathematical ideas used for computing "spline" curves can be extended to allow us to compute "spline" surfaces. The application ofthese mathematical ideas is rather widespread. Spline functions are central to computer graphics disciplines. Spline curves and surfaces are used in computer graphics renderings for both real and imagi­ nary objects. Computer-aided-design (CAD) systems depend on algorithms for computing spline functions, and splines are used in numerical analysis and statistics. Thus the construction of movies and computer games trav­ els side-by-side with the art of automobile design, sail construction, and architecture; and statisticians and applied mathematicians use splines as everyday computational tools, often divorced from graphic images.},
	language = {en},
	number = {1},
	urldate = {2020-11-11},
	publisher = {Birkhäuser Basel},
	author = {Knott, Gary D.},
	year = {2000},
	doi = {10.1007/978-1-4612-1320-8},
	file = {Snapshot:/Users/b7064522/Zotero/storage/7XGE9VZE/9781461270928.html:text/html},
}

@article{xiao_asymptotic_2020,
	title = {Asymptotic properties of penalized splines for functional data},
	volume = {26},
	issn = {1350-7265},
	url = {https://projecteuclid.org/euclid.bj/1598493633},
	doi = {10.3150/20-BEJ1209},
	abstract = {Penalized spline methods are popular for functional data analysis but their asymptotic properties have not been established. We present a theoretic study of the L2L2L\_\{2\} and uniform convergence of penalized splines for estimating the mean and covariance functions of functional data under general settings. The established convergence rates for the mean function estimation are mini-max rate optimal and the rates for the covariance function estimation are comparable to those using other smoothing methods.},
	language = {EN},
	number = {4},
	urldate = {2020-11-20},
	journal = {Bernoulli},
	author = {Xiao, Luo},
	month = nov,
	year = {2020},
	mrnumber = {MR4140531},
	zmnumber = {07256162},
	note = {Publisher: Bernoulli Society for Mathematical Statistics and Probability},
	pages = {2847--2875},
	file = {Snapshot:/Users/b7064522/Zotero/storage/R4PI8PWP/1598493633.html:text/html},
}

@article{xiao_asymptotic_2019,
	title = {Asymptotic theory of penalized splines},
	volume = {13},
	issn = {1935-7524},
	url = {https://projecteuclid.org/euclid.ejs/1553133771},
	doi = {10.1214/19-EJS1541},
	language = {EN},
	number = {1},
	urldate = {2020-11-20},
	journal = {Electron. J. Statist.},
	author = {Xiao, Luo},
	year = {2019},
	mrnumber = {MR3925516},
	zmnumber = {07056140},
	note = {Publisher: The Institute of Mathematical Statistics and the Bernoulli Society},
	pages = {747--794},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/DBPQH5EZ/Xiao - 2019 - Asymptotic theory of penalized splines.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/T35TBNBX/1553133771.html:text/html},
}

@article{zhou_local_1998,
	title = {Local {Asymptotics} for {Regression} {Splines} and {Confidence} {Regions}},
	volume = {26},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1024691356},
	doi = {10.1214/aos/1024691356},
	abstract = {In this paper, we study the local behavior of regression splines. In particular, explicit expressions for the asymptotic pointwise bias and variance of regression splines are obtained. In addition, asymptotic normality for regression splines is established, leading to the construction of approximate confidence intervals and confidence bands for the regression function.},
	language = {en},
	number = {5},
	urldate = {2020-11-20},
	journal = {Ann. Statist.},
	author = {Zhou, S. and Shen, X. and Wolfe, D. A.},
	month = oct,
	year = {1998},
	mrnumber = {MR1673277},
	zmnumber = {0929.62052},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {1760--1782},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/BGKAXKBM/Zhou et al. - 1998 - Local Asymptotics for Regression Splines and Confi.pdf:application/pdf},
}

@article{barrow_efficientl_1979,
	title = {{EfficientL} 2 approximation by splines},
	volume = {33},
	issn = {0029-599X, 0945-3245},
	url = {http://link.springer.com/10.1007/BF01396498},
	doi = {10.1007/BF01396498},
	language = {en},
	number = {1},
	urldate = {2020-11-23},
	journal = {Numer. Math.},
	author = {Barrow, D. L. and Smith, P. W.},
	month = mar,
	year = {1979},
	pages = {101--114},
}

@article{wood_low-rank_2006,
	title = {Low-rank scale-invariant tensor product smooths for generalized additive mixed models},
	volume = {62},
	issn = {0006-341X},
	doi = {10.1111/j.1541-0420.2006.00574.x},
	abstract = {A general method for constructing low-rank tensor product smooths for use as components of generalized additive models or generalized additive mixed models is presented. A penalized regression approach is adopted in which tensor product smooths of several variables are constructed from smooths of each variable separately, these "marginal" smooths being represented using a low-rank basis with an associated quadratic wiggliness penalty. The smooths offer several advantages: (i) they have one wiggliness penalty per covariate and are hence invariant to linear rescaling of covariates, making them useful when there is no "natural" way to scale covariates relative to each other; (ii) they have a useful tuneable range of smoothness, unlike single-penalty tensor product smooths that are scale invariant; (iii) the relatively low rank of the smooths means that they are computationally efficient; (iv) the penalties on the smooths are easily interpretable in terms of function shape; (v) the smooths can be generated completely automatically from any marginal smoothing bases and associated quadratic penalties, giving the modeler considerable flexibility to choose the basis penalty combination most appropriate to each modeling task; and (vi) the smooths can easily be written as components of a standard linear or generalized linear mixed model, allowing them to be used as components of the rich family of such models implemented in standard software, and to take advantage of the efficient and stable computational methods that have been developed for such models. A small simulation study shows that the methods can compare favorably with recently developed smoothing spline ANOVA methods.},
	language = {eng},
	number = {4},
	journal = {Biometrics},
	author = {Wood, Simon N.},
	month = dec,
	year = {2006},
	pmid = {17156276},
	pages = {1025--1036},
	file = {Accepted Version:/Users/b7064522/Zotero/storage/UMW4ICHL/Wood - 2006 - Low-rank scale-invariant tensor product smooths fo.pdf:application/pdf},
}

@book{abramowitz_handbook_2013,
	address = {New York, NY},
	series = {Dover books on mathematics},
	title = {Handbook of mathematical functions: with formulas, graphs, and mathematical tables},
	language = {eng},
	number = {1},
	publisher = {Dover Publ},
	editor = {Abramowitz, Milton and Stegun, Irene A.},
	year = {2013},
}

@article{kay_community_2015,
	title = {The {Community} {Earth} {System} {Model} ({CESM}) large ensemble project: a community resource for studying climate change in the presence of internal climate variability},
	volume = {96},
	issn = {0003-0007},
	shorttitle = {The {Community} {Earth} {System} {Model} ({CESM}) large ensemble project},
	doi = {10.1175/BAMS-D-13-00255.1},
	language = {English},
	number = {8},
	urldate = {2021-01-15},
	journal = {Bulletin of the American Meteorological Society},
	author = {Kay, Jennifer E. and Deser, Clara and Phillips, Adam S. and Mai, A. and Hannay, Cecile and Strand, Gary and Arblaster, Julie Michelle and Bates, S. C. and Danabasoglu, Gokhan and Edwards, James C. and Holland, Marika M. and Kushner, Paul J. and Lamarque, Jean-Francois and Lawrence, David M. and Lindsay, Keith and Middleton, A. and Munoz, Ernesto and Neale, Richard B. and Oleson, Keith W. and Polvani, Lorenzo M. and Vertenstein, Mariana},
	month = aug,
	year = {2015},
	note = {Publisher: American Meteorological Society},
	pages = {1333--1349},
	file = {Snapshot:/Users/b7064522/Zotero/storage/FVMGRI83/the-community-earth-system-model-cesm-large-ensemble-project-a-co.html:text/html;Submitted Version:/Users/b7064522/Zotero/storage/ADKYLLI7/Kay et al. - 2015 - The Community Earth System Model (CESM) large ense.pdf:application/pdf},
}

@article{paciorek_spatial_2006,
	title = {Spatial {Modelling} {Using} a {New} {Class} of {Nonstationary} {Covariance} {Functions}},
	volume = {17},
	issn = {1180-4009},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2157553/},
	doi = {10.1002/env.785},
	abstract = {We introduce a new class of nonstationary covariance functions for spatial modelling. Nonstationary covariance functions allow the model to adapt to spatial surfaces whose variability changes with location. The class includes a nonstationary version of the Matérn stationary covariance, in which the differentiability of the spatial surface is controlled by a parameter, freeing one from fixing the differentiability in advance. The class allows one to knit together local covariance parameters into a valid global nonstationary covariance, regardless of how the local covariance structure is estimated. We employ this new nonstationary covariance in a fully Bayesian model in which the unknown spatial process has a Gaussian process (GP) prior distribution with a nonstationary covariance function from the class. We model the nonstationary structure in a computationally efficient way that creates nearly stationary local behavior and for which stationarity is a special case. We also suggest non-Bayesian approaches to nonstationary kriging., To assess the method, we use real climate data to compare the Bayesian nonstationary GP model with a Bayesian stationary GP model, various standard spatial smoothing approaches, and nonstationary models that can adapt to function heterogeneity. The GP models outperform the competitors, but while the nonstationary GP gives qualitatively more sensible results, it shows little advantage over the stationary GP on held-out data, illustrating the difficulty in fitting complicated spatial data.},
	number = {5},
	urldate = {2021-01-18},
	journal = {Environmetrics},
	author = {Paciorek, Christopher J. and Schervish, Mark J.},
	year = {2006},
	pmid = {18163157},
	pmcid = {PMC2157553},
	pages = {483--506},
	file = {PubMed Central Full Text PDF:/Users/b7064522/Zotero/storage/R4NNZ57L/Paciorek and Schervish - 2006 - Spatial Modelling Using a New Class of Nonstationa.pdf:application/pdf},
}

@article{lukas_robust_2006,
	title = {Robust generalized cross-validation for choosing the regularization parameter},
	volume = {22},
	issn = {0266-5611},
	url = {https://doi.org/10.1088/0266-5611/22/5/021},
	doi = {10.1088/0266-5611/22/5/021},
	abstract = {Let fλ be the regularized solution for the problem of estimating a function or vector f0 from noisy data yi = Lif0 + εi, i = 1, …, n, where Li are linear functionals. A prominent method for the selection of the crucial regularization parameter λ is generalized cross-validation (GCV). It is known that GCV has good asymptotic properties as n → ∞ but it may not be reliable for small or medium sized n, sometimes giving an estimate that is far too small. We propose a new robust GCV method (RGCV) which chooses λ to be the minimizer of γV(λ) + (1 − γ)F(λ), where V(λ) is the GCV function, F(λ) is an approximate average measure of the influence of each data point on fλ and γ ∊ (0, 1) is a robustness parameter. We show that for any n, RGCV is less likely than GCV to choose a very small value of λ, resulting in a more robust method. We also show that RGCV has good asymptotic properties as n → ∞ for general linear operator equations with uncorrelated errors. The function EF(λ) approximates the risk ER(λ) for values of λ that are asymptotically a bit smaller than the minimizer of ER(λ) (where V(λ) may not approximate well). The ‘expected’ RGCV estimate is asymptotically optimal as n → ∞ with respect to the ‘robust risk’ γER(λ) + (1 − γ)v(λ), where v(λ) is the variance component of the risk, and it has the optimal decay rate with respect to ER(λ) and stronger error criteria. The GCV and RGCV methods are compared in numerical simulations for the problem of estimating the second derivative from noisy data. The results for RGCV with n = 51 are consistent with the asymptotic results, and, for a large range of γ values, RGCV is more reliable and accurate than GCV.},
	language = {en},
	number = {5},
	urldate = {2021-01-26},
	journal = {Inverse Problems},
	author = {Lukas, Mark A.},
	month = sep,
	year = {2006},
	note = {Publisher: IOP Publishing},
	pages = {1883--1902},
	file = {Full Text:/Users/b7064522/Zotero/storage/KB6URWZK/Lukas - 2006 - Robust generalized cross-validation for choosing t.pdf:application/pdf},
}

@article{sampson_nonparametric_1992,
	title = {Nonparametric {Estimation} of {Nonstationary} {Spatial} {Covariance} {Structure}},
	volume = {87},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2290458},
	doi = {10.2307/2290458},
	abstract = {Estimation of the covariance structure of spatial processes is a fundamental prerequisite for problems of spatial interpolation and the design of monitoring networks. We introduce a nonparametric approach to global estimation of the spatial covariance structure of a random function Z(x, t) observed repeatedly at times ti (i = 1, ⋯, T) at a finite number of sampling stations xi (i = 1, 2, ..., N) in the plane. Our analyses assume temporal stationarity but do not assume spatial stationarity (or isotropy). We analyze the spatial dispersions \${\textbackslash}operatorname\{var\}(Z(x\_i, t) - Z(x\_j, t))\$ as a natural metric for the spatial covariance structure and model these as a general smooth function of the geographic coordinate of station pairs (xi, xj). The model is constructed in two steps. First, using nonmetric multidimensional scaling (MDS) we compute a two-dimensional representation of the sampling stations for which a monotone function of interpoint distances δij approximates the spatial dispersions. MDS transforms the problem into one for which the covariance structure, expressed in terms of spatial dispersions, is stationary and isotropic. Second, we compute thinplate splines to provide smooth mappings of the geographic representation of the sampling stations into their MDS representation. The composition of this mapping f and a monotone function g derived from MDS yields a nonparametric estimator of \${\textbackslash}operatorname\{var\}(Z(x\_a, t) - Z(x\_b, t))\$ for any two geographic locations xa and xb (monitored or not) of the form g(∣ f(xa) - f(xb(∣). By restricting the monotone function g to a class of conditionally nonpositive definite variogram functions, we ensure that the resulting nonparametric model corresponds to a nonnegative definite covariance model. We use biorthogonal grids, introduced by Bookstein in the field of morphometrics, to depict the thin-plate spline mappings that embody the nature of the anisotropy and nonstationarity in the sample covariance matrix. An analysis of mesoscale variability in solar radiation monitored in southwestern British Columbia demonstrates this methodology.},
	number = {417},
	urldate = {2021-01-29},
	journal = {Journal of the American Statistical Association},
	author = {Sampson, Paul D. and Guttorp, Peter},
	year = {1992},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {108--119},
	file = {JSTOR Full Text PDF:/Users/b7064522/Zotero/storage/3MX4W28Q/Sampson and Guttorp - 1992 - Nonparametric Estimation of Nonstationary Spatial .pdf:application/pdf},
}

@book{wood_generalized_2006,
	address = {Philadelphia, PA, UNITED STATES},
	title = {Generalized {Additive} {Models}: {An} {Introduction} with {R}},
	isbn = {978-1-4200-1040-4},
	shorttitle = {Generalized {Additive} {Models}},
	url = {http://ebookcentral.proquest.com/lib/ncl/detail.action?docID=1633306},
	urldate = {2021-02-26},
	publisher = {CRC Press LLC},
	author = {Wood, Simon N.},
	year = {2006},
	file = {ProQuest Ebook Snapshot:/Users/b7064522/Zotero/storage/5WFN4D4X/reader.html:text/html},
}

@inproceedings{higdon_space_2002,
	address = {London},
	title = {Space and {Space}-{Time} {Modeling} using {Process} {Convolutions}},
	isbn = {978-1-4471-0657-9},
	doi = {10.1007/978-1-4471-0657-9_2},
	abstract = {A continuous spatial model can be constructed by convolving a very simple, perhaps independent, process with a kernel or point spread function. This approach for constructing a spatial process offers a number of advantages over specification through a spatial covariogram. In particular, this process convolution specification leads to computational simplifications and easily extends beyond simple stationary models. This paper uses process convolution models to build space and space-time models that are flexible and able to accommodate large amounts of data. Data from environmental monitoring is considered.},
	language = {en},
	booktitle = {Quantitative {Methods} for {Current} {Environmental} {Issues}},
	publisher = {Springer},
	author = {Higdon, Dave},
	editor = {Anderson, Clive W. and Barnett, Vic and Chatwin, Philip C. and El-Shaarawi, Abdel H.},
	year = {2002},
	pages = {37--56},
}

@article{lindgren_explicit_2011,
	title = {An explicit link between {Gaussian} fields and {Gaussian} {Markov} random fields: the stochastic partial differential equation approach},
	volume = {73},
	copyright = {© 2011 Royal Statistical Society},
	issn = {1467-9868},
	shorttitle = {An explicit link between {Gaussian} fields and {Gaussian} {Markov} random fields},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2011.00777.x},
	doi = {https://doi.org/10.1111/j.1467-9868.2011.00777.x},
	abstract = {Summary. Continuously indexed Gaussian fields (GFs) are the most important ingredient in spatial statistical modelling and geostatistics. The specification through the covariance function gives an intuitive interpretation of the field properties. On the computational side, GFs are hampered with the big n problem, since the cost of factorizing dense matrices is cubic in the dimension. Although computational power today is at an all time high, this fact seems still to be a computational bottleneck in many applications. Along with GFs, there is the class of Gaussian Markov random fields (GMRFs) which are discretely indexed. The Markov property makes the precision matrix involved sparse, which enables the use of numerical algorithms for sparse matrices, that for fields in only use the square root of the time required by general algorithms. The specification of a GMRF is through its full conditional distributions but its marginal properties are not transparent in such a parameterization. We show that, using an approximate stochastic weak solution to (linear) stochastic partial differential equations, we can, for some GFs in the Matérn class, provide an explicit link, for any triangulation of , between GFs and GMRFs, formulated as a basis function representation. The consequence is that we can take the best from the two worlds and do the modelling by using GFs but do the computations by using GMRFs. Perhaps more importantly, our approach generalizes to other covariance functions generated by SPDEs, including oscillating and non-stationary GFs, as well as GFs on manifolds. We illustrate our approach by analysing global temperature data with a non-stationary model defined on a sphere.},
	language = {en},
	number = {4},
	urldate = {2021-04-06},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Lindgren, Finn and Rue, Håvard and Lindström, Johan},
	year = {2011},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2011.00777.x},
	pages = {423--498},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/VG95C42T/Lindgren et al. - 2011 - An explicit link between Gaussian fields and Gauss.pdf:application/pdf},
}

@article{hurrell_community_2013,
	title = {The {Community} {Earth} {System} {Model}: {A} {Framework} for {Collaborative} {Research}},
	volume = {94},
	shorttitle = {The {Community} {Earth} {System} {Model}},
	url = {https://journals.ametsoc.org/view/journals/bams/94/9/bams-d-12-00121.1.xml},
	doi = {10.1175/BAMS-D-12-00121.1},
	abstract = {{\textless}section class="abstract"{\textgreater}{\textless}p{\textgreater}The Community Earth System Model (CESM) is a flexible and extensible community tool used to investigate a diverse set of Earth system interactions across multiple time and space scales. This global coupled model significantly extends its predecessor, the Community Climate System Model, by incorporating new Earth system simulation capabilities. These comprise the ability to simulate biogeochemical cycles, including those of carbon and nitrogen, a variety of atmospheric chemistry options, the Greenland Ice Sheet, and an atmosphere that extends to the lower thermosphere. These and other new model capabilities are enabling investigations into a wide range of pressing scientific questions, providing new foresight into possible future climates and increasing our collective knowledge about the behavior and interactions of the Earth system. Simulations with numerous configurations of the CESM have been provided to phase 5 of the Coupled Model Intercomparison Project (CMIP5) and are being analyzed by the broad community of scientists. Additionally, the model source code and associated documentation are freely available to the scientific community to use for Earth system studies, making it a true community tool. This article describes this Earth system model and its various possible configurations, and highlights a number of its scientific capabilities.{\textless}/p{\textgreater}{\textless}/section{\textgreater}},
	language = {EN},
	number = {9},
	urldate = {2021-04-06},
	journal = {Bulletin of the American Meteorological Society},
	author = {Hurrell, James W. and Holland, M. M. and Gent, P. R. and Ghan, S. and Kay, Jennifer E. and Kushner, P. J. and Lamarque, J.-F. and Large, W. G. and Lawrence, D. and Lindsay, K. and Lipscomb, W. H. and Long, M. C. and Mahowald, N. and Marsh, D. R. and Neale, R. B. and Rasch, P. and Vavrus, S. and Vertenstein, M. and Bader, D. and Collins, W. D. and Hack, J. J. and Kiehl, J. and Marshall, S.},
	month = sep,
	year = {2013},
	note = {Publisher: American Meteorological Society
Section: Bulletin of the American Meteorological Society},
	pages = {1339--1360},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/KKHZ3DAW/Hurrell et al. - 2013 - The Community Earth System Model A Framework for .pdf:application/pdf},
}

@article{neale_description_nodate,
	title = {Description of the {NCAR} {Community} {Atmosphere} {Model} ({CAM} 5.0)},
	language = {en},
	author = {Neale, Richard B and Gettelman, Andrew and Park, Sungsu and Chen, Chih-Chieh and Lauritzen, Peter H and Williamson, David L and Conley, Andrew J and Kinnison, Doug and Marsh, Dan and Smith, Anne K and Vitt, Francis and Garcia, Rolando and Lamarque, Jean-Francois and Mills, Mike and Tilmes, Simone and Morrison, Hugh and Cameron-Smith, Philip and Collins, William D and Iacono, Michael J and Easter, Richard C and Liu, Xiaohong and Ghan, Steven J and Rasch, Philip J and Taylor, Mark A},
	pages = {289},
	file = {Neale et al. - Description of the NCAR Community Atmosphere Model.pdf:/Users/b7064522/Zotero/storage/CQEBU8IX/Neale et al. - Description of the NCAR Community Atmosphere Model.pdf:application/pdf},
}

@article{fan_study_1996,
	title = {A {STUDY} {OF} {VARIABLE} {BANDWIDTH} {SELECTION} {FOR} {LOCAL} {POLYNOMIAL} {REGRESSION}},
	volume = {6},
	issn = {1017-0405},
	url = {https://www.jstor.org/stable/24306002},
	abstract = {A decisive question in nonparametric smoothing techniques is the choice of the bandwidth or smoothing parameter. The present paper addresses this question when using local polynomial approximations for estimating the regression function and its derivatives. A fully-automatic bandwidth selection procedure has been proposed by Fan and Gijbels (1995a), and the empirical performance of it was tested in detail via a variety of examples. Those experiences supported the methodology towards a great extend. In this paper we establish asymptotic results for the proposed variable bandwidth selector. We provide the rate of convergence of the bandwidth estimate, and obtain the asymptotic distribution of its error relative to the theoretical optimal variable bandwidth. These asymptotic properties give extra support to the proposed bandwidth selection procedure. It is also demonstrated how the proposed selection method can be applied in the density estimation setup. some examples illustrate this application.},
	number = {1},
	urldate = {2021-04-08},
	journal = {Statistica Sinica},
	author = {Fan, Jianqing and Gijbels, Irène and Hu, Tien-Chung and Huang, Li-Shan},
	year = {1996},
	note = {Publisher: Institute of Statistical Science, Academia Sinica},
	pages = {113--127},
}

@book{bjorck_numerical_1996,
	address = {Philadelphia},
	title = {Numerical methods for least squares problems},
	isbn = {978-0-89871-360-2},
	publisher = {SIAM},
	author = {Björck, Ake},
	year = {1996},
}

@article{osullivan_statistical_1986,
	title = {A {Statistical} {Perspective} on {Ill}-{Posed} {Inverse} {Problems}},
	volume = {1},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-1/issue-4/A-Statistical-Perspective-on-Ill-Posed-Inverse-Problems/10.1214/ss/1177013525.full},
	doi = {10.1214/ss/1177013525},
	abstract = {Ill-posed inverse problems arise in many branches of science and engineering. In the typical situation one is interested in recovering a whole function given a finite number of noisy measurements on functionals. Performance characteristics of an inversion algorithm are studied via the mean square error which is decomposed into bias and variability. Variability calculations are often straightforward, but useful bias measures are more difficult to obtain. An appropriate definition of what geophysicists call the Backus-Gilbert averaging kernel leads to a natural way of measuring bias characteristics. Moreover, the ideas give rise to some important experimental design criteria. It can be shown that the optimal inversion algorithms are methods of regularization procedures, but to completely specify these algorithms the signal to noise ratio must be supplied. Statistical approaches to the empirical determination of the signal to noise ratio are discussed; cross-validation and unbiased risk methods are reviewed; and some extensions, which seem particularly appropriate in the inverse problem context, are indicated. Linear and nonlinear examples from medicine, meteorology, and geophysics are used for illustration.},
	number = {4},
	urldate = {2021-04-09},
	journal = {Statistical Science},
	author = {O'Sullivan, Finbarr},
	month = nov,
	year = {1986},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {502--518},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/UKCMETMU/O'Sullivan - 1986 - A Statistical Perspective on Ill-Posed Inverse Pro.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/BYJJXN93/1177013525.html:text/html},
}

@book{ruppert_semiparametric_2003,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {Semiparametric {Regression}},
	isbn = {978-0-521-78050-6},
	url = {https://www.cambridge.org/core/books/semiparametric-regression/02FC9A9435232CA67532B4D31874412C},
	abstract = {Semiparametric regression is concerned with the flexible incorporation of non-linear functional relationships in regression analyses. Any application area that benefits from regression analysis can also benefit from semiparametric regression. Assuming only a basic familiarity with ordinary parametric regression, this user-friendly book explains the techniques and benefits of semiparametric regression in a concise and modular fashion. The authors make liberal use of graphics and examples plus case studies taken from environmental, financial, and other applications. They include practical advice on implementation and pointers to relevant software. The 2003 book is suitable as a textbook for students with little background in regression as well as a reference book for statistically oriented scientists such as biostatisticians, econometricians, quantitative social scientists, epidemiologists, with a good working knowledge of regression and the desire to begin using more flexible semiparametric models. Even experts on semiparametric regression should find something new here.},
	urldate = {2021-04-09},
	publisher = {Cambridge University Press},
	author = {Ruppert, David and Wand, M. P. and Carroll, R. J.},
	year = {2003},
	doi = {10.1017/CBO9780511755453},
	file = {Snapshot:/Users/b7064522/Zotero/storage/YCKSQ2BR/02FC9A9435232CA67532B4D31874412C.html:text/html},
}

@article{wahba_practical_1977,
	title = {Practical {Approximate} {Solutions} to {Linear} {Operator} {Equations} when the {Data} are {Noisy}},
	volume = {14},
	issn = {0036-1429},
	url = {https://www.jstor.org/stable/2156485},
	abstract = {We consider approximate solutions fn,λ to linear operator equations Kf = g, of the form: fn,λ is the minimizer in H of (1/n)∑n j = 1 [(Kh)(tj) - y(tj)]2 + λ {\textbar} h {\textbar}2, where H is a Hilbert space, and the data \{y(tj)\} satisfy y(tj) = g(tj) + ε(tj), the \{ε(tj)\} being measurement errors. fn,λ is the so-called regularized solution, and \${\textbackslash}lambda {\textgreater} 0\$ is the regularization parameter, to be chosen. It is important to choose λ correctly. The purpose of this paper is to propose the method of weighted cross-validation for choosing λ from the data. We suppose that g is very smooth and the errors are white noise. It is shown that the weighted cross-validation estimate \${\textbackslash}hat {\textbackslash}lambda\$ estimates the value of λ which minimizes (1/n)E∑n j = 1 [(Kfn,λ)(tj) - (Kf)(tj)]2. Results related to the convergence of \${\textbackslash}{\textbar}f - f\_\{n,{\textbackslash}hat {\textbackslash}lambda\}{\textbackslash}{\textbar}\$, including rates, are obtained.},
	number = {4},
	urldate = {2021-04-12},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Wahba, Grace},
	year = {1977},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {651--667},
}

@article{wahba_comparison_1985,
	title = {A {Comparison} of {GCV} and {GML} for {Choosing} the {Smoothing} {Parameter} in the {Generalized} {Spline} {Smoothing} {Problem}},
	volume = {13},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-13/issue-4/A-Comparison-of-GCV-and-GML-for-Choosing-the-Smoothing/10.1214/aos/1176349743.full},
	doi = {10.1214/aos/1176349743},
	abstract = {The partially improper prior behind the smoothing spline model is used to obtain a generalization of the maximum likelihood (GML) estimate for the smoothing parameter. Then this estimate is compared with the generalized cross validation (GCV) estimate both analytically and by Monte Carlo methods. The comparison is based on a predictive mean square error criteria. It is shown that if the true, unknown function being estimated is smooth in a sense to be defined then the GML estimate undersmooths relative to the GCV estimate and the predictive mean square error using the GML estimate goes to zero at a slower rate than the mean square error using the GCV estimate. If the true function is "rough" then the GCV and GML estimates have asymptotically similar behavior. A Monte Carlo experiment was designed to see if the asymptotic results in the smooth case were evident in small sample sizes. Mixed results were obtained for \$n = 32\$, GCV was somewhat better than GML for \$n = 64\$, and GCV was decidedly superior for \$n = 128\$. In the \$n = 32\$ case GCV was better for smaller \${\textbackslash}sigma{\textasciicircum}2\$ and the comparison close for larger \${\textbackslash}sigma{\textasciicircum}2\$. The theoretical results are shown to extend to the generalized spline smoothing model, which includes the estimate of functions given noisy values of various integrals of them.},
	number = {4},
	urldate = {2021-04-12},
	journal = {The Annals of Statistics},
	author = {Wahba, Grace},
	month = dec,
	year = {1985},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {1378--1402},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/TI9G4RYP/Wahba - 1985 - A Comparison of GCV and GML for Choosing the Smoot.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/KH8WNSEP/1176349743.html:text/html},
}

@misc{noauthor_performance_nodate,
	title = {Performance of {Robust} {GCV} and {Modified} {GCV} for {Spline} {Smoothing} - {LUKAS} - 2012 - {Scandinavian} {Journal} of {Statistics} - {Wiley} {Online} {Library}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2011.00736.x},
	urldate = {2021-04-12},
	file = {Performance of Robust GCV and Modified GCV for Spline Smoothing - LUKAS - 2012 - Scandinavian Journal of Statistics - Wiley Online Library:/Users/b7064522/Zotero/storage/3FUBS5C2/j.1467-9469.2011.00736.html:text/html},
}

@article{lukas_performance_2012,
	title = {Performance of {Robust} {GCV} and {Modified} {GCV} for {Spline} {Smoothing}},
	volume = {39},
	copyright = {© 2011 Board of the Foundation of the Scandinavian Journal of Statistics},
	issn = {1467-9469},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2011.00736.x},
	doi = {https://doi.org/10.1111/j.1467-9469.2011.00736.x},
	abstract = {Abstract. While it is a popular selection criterion for spline smoothing, generalized cross-validation (GCV) occasionally yields severely undersmoothed estimates. Two extensions of GCV called robust GCV (RGCV) and modified GCV have been proposed as more stable criteria. Each involves a parameter that must be chosen, but the only guidance has come from simulation results. We investigate the performance of the criteria analytically. In most studies, the mean square prediction error is the only loss function considered. Here, we use both the prediction error and a stronger Sobolev norm error, which provides a better measure of the quality of the estimate. A geometric approach is used to analyse the superior small-sample stability of RGCV compared to GCV. In addition, by deriving the asymptotic inefficiency for both the prediction error and the Sobolev error, we find intervals for the parameters of RGCV and modified GCV for which the criteria have optimal performance.},
	language = {en},
	number = {1},
	urldate = {2021-04-12},
	journal = {Scandinavian Journal of Statistics},
	author = {Lukas, Mark A. and Hoog, Frank R. De and Anderssen, Robert S.},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9469.2011.00736.x},
	pages = {97--115},
	file = {Full Text:/Users/b7064522/Zotero/storage/A2XIARHI/Lukas et al. - 2012 - Performance of Robust GCV and Modified GCV for Spl.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/7LPBTZD9/j.1467-9469.2011.00736.html:text/html},
}

@article{cummins_confidence_2001,
	title = {Confidence {Intervals} for {Nonparametric} {Curve} {Estimates}: {Toward} {More} {Uniform} {Pointwise} {Coverage}},
	volume = {96},
	issn = {0162-1459},
	shorttitle = {Confidence {Intervals} for {Nonparametric} {Curve} {Estimates}},
	url = {https://www.jstor.org/stable/2670362},
	abstract = {Numerous nonparametric regression methods exist that yield consistent estimators of function curves. Often, one is also interested in constructing confidence intervals for the unknown function. When a function estimate is based on a single global smoothing parameter the resulting confidence intervals may hold their desired confidence level 1 -α on average but because bias in nonparametric estimation is not uniform, they do not hold the desired level uniformly at all design points. Most research in this area has focused on mean squared error properties of the estimator, for example MISE, itself a global measure. In addition, measures like MISE are one step removed from the practical issue of coverage probability. Recent work that focuses on coverage probability has considered only coverage in an average sense, ignoring the important issue of uniformity of coverage across the design space. To deal with this problem, a new estimator is developed which uses a local cross-validation criterion (LCV) to determine a separate smoothing parameter for each design point. The local smoothing parameters are then used to compute the point estimators of the regression curve and the corresponding pointwise confidence intervals. Incorporation of local information through the new method is shown, via Monte Carlo simulation, to yield more uniformly valid pointwise confidence intervals for nonparametric regression curves. Diagnostic plots are developed (Breakout Plots) to visually inspect the degree of uniformity of coverage of the confidence intervals. The approach, here applied to cubic smoothing splines, easily generalizes to many other nonparametric regression estimators. The improved curve estimation is not a solely theoretical improvement such as providing an estimator that has a faster EASE convergence rate but shows its worth empirically by yielding improved coverage probabilities through reliable pointwise confidence intervals.},
	number = {453},
	urldate = {2021-04-12},
	journal = {Journal of the American Statistical Association},
	author = {Cummins, David J. and Filloon, Tom G. and Nychka, Douglas},
	year = {2001},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {233--246},
}

@article{aguilera_forecasting_1999,
	title = {Forecasting time series by functional {PCA}. {Discussion} of several weighted approaches},
	volume = {14},
	issn = {1613-9658},
	url = {https://doi.org/10.1007/s001800050025},
	doi = {10.1007/s001800050025},
	abstract = {In this paper a functional principal component model is applied to forecast a continuous time series that has been observed only at discrete time points not necessarily equally spaced. To take into account the natural order among the sample paths obtained after cutting the series into pieces, a weighted estimation of the principal components is proposed. In order to estimate the weighted functional principal component analysis, a cubic spline interpolation of the sample paths between their discrete observations is performed. Finally, an application with simulated data is developed where model fitting and forecasting results using different types of weightings on equally and unequally spaced data are given and discussed. The forecasting performance of the estimated functional principal component models is also compared with multivariate principal component regression models.},
	language = {en},
	number = {3},
	urldate = {2021-04-12},
	journal = {Computational Statistics},
	author = {Aguilera, Ana M. and Ocaña, Francisco A. and Valderrama, Mariano J.},
	month = sep,
	year = {1999},
	pages = {443--467},
	file = {Springer Full Text PDF:/Users/b7064522/Zotero/storage/SD5L5BWZ/Aguilera et al. - 1999 - Forecasting time series by functional PCA. Discuss.pdf:application/pdf},
}

@article{hyndman_forecasting_2009,
	title = {Forecasting functional time series},
	abstract = {We propose forecasting functional time series using weighted functional principal component regression and weighted functional partial least squares regression. These approaches allow for smooth functions, assign higher weights to more recent data, and provide a modeling scheme that is easily adapted to allow for constraints and other information. We illustrate our approaches using age-specific French female mortality rates from 1816 to 2006 and age-specific Australian fertility rates from 1921 to 2006, and show that these weighted methods improve forecast accuracy in comparison to their unweighted counterparts. We also propose two new bootstrap methods to construct prediction intervals, and evaluate and compare their empirical coverage probabilities.},
	language = {en},
	journal = {Journal of the Korean Statistical Society},
	author = {Hyndman, Rob J and Shang, Han Lin},
	year = {2009},
	pages = {13},
	file = {Hyndman and Shang - 2009 - Forecasting functional time series.pdf:/Users/b7064522/Zotero/storage/8WV3JDWS/Hyndman and Shang - 2009 - Forecasting functional time series.pdf:application/pdf},
}

@article{hyndman_stochastic_2008,
	title = {Stochastic population forecasts using functional data models for mortality, fertility and migration},
	volume = {24},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207008000319},
	doi = {10.1016/j.ijforecast.2008.02.009},
	abstract = {Age–sex-specific population forecasts are derived through stochastic population renewal using forecasts of mortality, fertility and net migration. Functional data models with time series coefficients are used to model age-specific mortality and fertility rates. As detailed migration data are lacking, net migration by age and sex is estimated as the difference between historic annual population data and successive populations one year ahead derived from a projection using fertility and mortality data. This estimate, which includes error, is also modeled using a functional data model. The three models involve different strengths of the general Box–Cox transformation chosen to minimise out-of-sample forecast error. Uncertainty is estimated from the model, with an adjustment to ensure that the one-step-forecast variances are equal to those obtained with historical data. The three models are then used in a Monte Carlo simulation of future fertility, mortality and net migration, which are combined using the cohort-component method to obtain age-specific forecasts of the population by sex. The distribution of the forecasts provides probabilistic prediction intervals. The method is demonstrated by making 20-year forecasts using Australian data for the period 1921–2004. The advantages of our method are: (1) it is a coherent stochastic model of the three demographic components; (2) it is estimated entirely from historical data with no subjective inputs required; and (3) it provides probabilistic prediction intervals for any demographic variable that is derived from population numbers and vital events, including life expectancies, total fertility rates and dependency ratios.},
	language = {en},
	number = {3},
	urldate = {2021-04-12},
	journal = {International Journal of Forecasting},
	author = {Hyndman, Rob J. and Booth, Heather},
	month = jul,
	year = {2008},
	pages = {323--342},
	file = {ScienceDirect Full Text PDF:/Users/b7064522/Zotero/storage/PPLTG9ZF/Hyndman and Booth - 2008 - Stochastic population forecasts using functional d.pdf:application/pdf;ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/7NHDZLBM/S0169207008000319.html:text/html},
}

@article{hyndman_robust_2007,
	title = {Robust forecasting of mortality and fertility rates: {A} functional data approach},
	volume = {51},
	issn = {0167-9473},
	shorttitle = {Robust forecasting of mortality and fertility rates},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947306002453},
	doi = {10.1016/j.csda.2006.07.028},
	abstract = {A new method is proposed for forecasting age-specific mortality and fertility rates observed over time. This approach allows for smooth functions of age, is robust for outlying years due to wars and epidemics, and provides a modelling framework that is easily adapted to allow for constraints and other information. Ideas from functional data analysis, nonparametric smoothing and robust statistics are combined to form a methodology that is widely applicable to any functional time series data observed discretely and possibly with error. The model is a generalization of the Lee–Carter (LC) model commonly used in mortality and fertility forecasting. The methodology is applied to French mortality data and Australian fertility data, and the forecasts obtained are shown to be superior to those from the LC method and several of its variants.},
	language = {en},
	number = {10},
	urldate = {2021-04-12},
	journal = {Computational Statistics \& Data Analysis},
	author = {Hyndman, Rob J. and Shahid Ullah, Md.},
	month = jun,
	year = {2007},
	pages = {4942--4956},
	file = {ScienceDirect Full Text PDF:/Users/b7064522/Zotero/storage/QYAK96ZW/Hyndman and Shahid Ullah - 2007 - Robust forecasting of mortality and fertility rate.pdf:application/pdf;ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/TWEZPQQT/S0167947306002453.html:text/html},
}

@book{billingsley_probability_1995,
	title = {Probability and {Measure}},
	isbn = {978-0-471-00710-4},
	abstract = {PROBABILITY AND MEASURE  Third Edition  Now in its new third edition, Probability and Measure offers advanced students, scientists, and engineers an integrated introduction to measure theory and probability. Retaining the unique approach of the previous editions, this text interweaves material on probability and measure, so that probability problems generate an interest in measure theory and measure theory is then developed and applied to probability. Probability and Measure provides thorough coverage of probability, measure, integration, random variables and expected values, convergence of distributions, derivatives and conditional probability, and stochastic processes. The Third Edition features an improved treatment of Brownian motion and the replacement of queuing theory with ergodic theory.  Like the previous editions, this new edition will be well received by students of mathematics, statistics, economics, and a wide variety of disciplines that require a solid understanding of probability theory.},
	language = {en},
	publisher = {Wiley},
	author = {Billingsley, Patrick},
	month = may,
	year = {1995},
	note = {Google-Books-ID: z39jQgAACAAJ},
}

@article{kaiser_varimax_1958,
	title = {The varimax criterion for analytic rotation in factor analysis},
	volume = {23},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02289233},
	doi = {10.1007/BF02289233},
	abstract = {An analytic criterion for rotation is defined. The scientific advantage of analytic criteria over subjective (graphical) rotational procedures is discussed. Carroll's criterion and the quartimax criterion are briefly reviewed; the varimax criterion is outlined in detail and contrasted both logically and numerically with the quartimax criterion. It is shown that thenormal varimax solution probably coincides closely to the application of the principle of simple structure. However, it is proposed that the ultimate criterion of a rotational procedure is factorial invariance, not simple structure—although the two notions appear to be highly related. The normal varimax criterion is shown to be a two-dimensional generalization of the classic Spearman case, i.e., it shows perfect factorial invariance for two pure clusters. An example is given of the invariance of a normal varimax solution for more than two factors. The oblique normal varimax criterion is stated. A computational outline for the orthogonal normal varimax is appended.},
	language = {en},
	number = {3},
	urldate = {2021-05-11},
	journal = {Psychometrika},
	author = {Kaiser, Henry F.},
	month = sep,
	year = {1958},
	pages = {187--200},
	file = {Springer Full Text PDF:/Users/b7064522/Zotero/storage/CXYKUQVA/Kaiser - 1958 - The varimax criterion for analytic rotation in fac.pdf:application/pdf},
}

@article{wilson_gaussian_2013,
	title = {Gaussian {Process} {Kernels} for {Pattern} {Discovery} and {Extrapolation}},
	copyright = {Andrew Gordon Wilson; Ryan Prescott Adams},
	issn = {1532-4435},
	url = {https://dash.harvard.edu/handle/1/11337457},
	abstract = {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modeling a spectral density – the Fourier transform of a kernel – with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data. We also show that it is possible to reconstruct several popular standard covariances within our framework.},
	language = {en\_US},
	urldate = {2021-05-11},
	journal = {Journal of Machine Learning Research},
	author = {Wilson, Andrew Gordon and Adams, Ryan Prescott},
	year = {2013},
	note = {Accepted: 2013-11-25T20:46:27Z
Publisher: Microtome Publishing},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/4UESZIG4/Wilson and Adams - 2013 - Gaussian Process Kernels for Pattern Discovery and.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/LTLLSTED/11337457.html:text/html},
}

@article{wang_mean_2009,
	title = {Mean squared error: {Love} it or leave it? {A} new look at {Signal} {Fidelity} {Measures}},
	volume = {26},
	issn = {1558-0792},
	shorttitle = {Mean squared error},
	doi = {10.1109/MSP.2008.930649},
	abstract = {In this article, we have reviewed the reasons why we (collectively) want to love or leave the venerable (but perhaps hoary) MSE. We have also reviewed emerging alternative signal fidelity measures and discussed their potential application to a wide variety of problems. The message we are trying to send here is not that one should abandon use of the MSE nor to blindly switch to any other particular signal fidelity measure. Rather, we hope to make the point that there are powerful, easy-to-use, and easy-to-understand alternatives that might be deployed depending on the application environment and needs. While we expect (and indeed, hope) that the MSE will continue to be widely used as a signal fidelity measure, it is our greater desire to see more advanced signal fidelity measures being used, especially in applications where perceptual criteria might be relevant. Ideally, the performance of a new signal processing algorithm might be compared to other algorithms using several fidelity criteria. Lastly, we hope that we have given further motivation to the community to consider recent advanced signal fidelity measures as design criteria for optimizing signal processing algorithms and systems. It is in this direction that we believe that the greatest benefit eventually lies.},
	number = {1},
	journal = {IEEE Signal Processing Magazine},
	author = {Wang, Zhou and Bovik, Alan C.},
	month = jan,
	year = {2009},
	note = {Conference Name: IEEE Signal Processing Magazine},
	pages = {98--117},
	file = {IEEE Xplore Abstract Record:/Users/b7064522/Zotero/storage/JE6W7FR9/4775883.html:text/html},
}

@article{wang_image_2004,
	title = {Image quality assessment: from error visibility to structural similarity},
	volume = {13},
	issn = {1941-0042},
	shorttitle = {Image quality assessment},
	doi = {10.1109/TIP.2003.819861},
	abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	month = apr,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Image Processing},
	pages = {600--612},
	file = {IEEE Xplore Abstract Record:/Users/b7064522/Zotero/storage/A7WN4A4B/1284395.html:text/html},
}

@book{fletcher_practical_1987,
	title = {Practical methods of optimization},
	isbn = {978-0-471-49463-8 978-0-471-91547-8},
	url = {http://archive.org/details/practicalmethods0000flet},
	abstract = {xiv, 436 p. : 24 cm; "A Wiley-Interscience publication."; Bibliography: p. [417]-429; Includes index},
	language = {eng},
	urldate = {2021-06-24},
	publisher = {Chichester ; New York : Wiley},
	author = {Fletcher, R. (Roger)},
	collaborator = {{Internet Archive}},
	year = {1987},
}

@article{anderes_local_2011,
	title = {Local likelihood estimation for nonstationary random fields},
	volume = {102},
	issn = {0047-259X},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X10002162},
	doi = {10.1016/j.jmva.2010.10.010},
	abstract = {We develop a weighted local likelihood estimate for the parameters that govern the local spatial dependency of a locally stationary random field. The advantage of this local likelihood estimate is that it smoothly downweights the influence of faraway observations, works for irregular sampling locations, and when designed appropriately, can trade bias and variance for reducing estimation error. This paper starts with an exposition of our technique on the problem of estimating an unknown positive function when multiplied by a stationary random field. This example gives concrete evidence of the benefits of our local likelihood as compared to unweighted local likelihoods. We then discuss the difficult problem of estimating a bandwidth parameter that controls the amount of influence from distant observations. Finally we present a simulation experiment for estimating the local smoothness of a local Matérn random field when observing the field at random sampling locations in [0,1]2. The local Matérn is a fully nonstationary random field, has a closed form covariance, can attain any degree of differentiability or Hölder smoothness and behaves locally like a stationary Matérn. We include an appendix that proves the positive definiteness of this covariance function.},
	language = {en},
	number = {3},
	urldate = {2021-07-23},
	journal = {Journal of Multivariate Analysis},
	author = {Anderes, Ethan B. and Stein, Michael L.},
	month = mar,
	year = {2011},
	pages = {506--520},
	file = {ScienceDirect Full Text PDF:/Users/b7064522/Zotero/storage/MPLQGYGS/Anderes and Stein - 2011 - Local likelihood estimation for nonstationary rand.pdf:application/pdf;ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/2X7547QQ/S0047259X10002162.html:text/html},
}

@phdthesis{gibbs_bayesian_1998,
	title = {Bayesian {Gaussian} {Processes} for {Regression} and {Classi} cation},
	abstract = {Bayesian inference offers us a powerful tool with which to tackle the problem of data modelling. However, the performance of Bayesian methods is crucially dependent on being able to find good models for our data. The principal focus of this thesis is the development of models based on Gaussian process priors. Such models, which can be thought of as the infinite extension of several existing finite models, have the flexibility to model complex phenomena while being mathematically simple. In thesis, I present a review of the theory of Gaussian processes and their covariance functions and demonstrate how they fit into the Bayesian framework. The efficient implementation of a Gaussian process is discussed with particular reference to approximate methods for matrix inversion based on the work of Skilling (1993). Several regression problems are examined. Non-stationary covariance functions are developed for the regression of neuron spike data and the use of Gaussian processes to model the potential energy surfaces of weakly bound molecules is discussed. Classification methods based on Gaussian processes are implemented using variational methods. Existing bounds (Jaakkola and Jordan 1996) for the sigmoid function are used to tackle binary problems and multi-dimensional bounds on the softmax function are presented for the multiple class case. The performance of the variational classifier is compared with that of other methods using the CRABS and PIMA datasets (Ripley 1996) and the problem of predicting the cracking of welds based on their chemical composition is also investigated. The theoretical calculation of the density of states of crystal structures is discussed in detail. Three possible approaches to the problem are described based on free energy minimization, Gaussian processes and the theory of random matrices. Results from these approaches are compared with the state-of-the-art techniques (Pickard 1997)},
	school = {University of Cambridge},
	author = {Gibbs, Mark},
	year = {1998},
}

@article{guinness_isotropic_2016,
	title = {Isotropic covariance functions on spheres: {Some} properties and modeling considerations},
	volume = {143},
	issn = {0047-259X},
	shorttitle = {Isotropic covariance functions on spheres},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X15002109},
	doi = {10.1016/j.jmva.2015.08.018},
	abstract = {Introducing flexible covariance functions is critical for interpolating spatial data since the properties of interpolated surfaces depend on the covariance function used for Kriging. An extensive literature is devoted to covariance functions on Euclidean spaces, where the Matérn covariance family is a valid and flexible parametric family capable of controlling the smoothness of corresponding stochastic processes. Many applications in environmental statistics involve data located on spheres, where less is known about properties of covariance functions, and where the Matérn is not generally a valid model with great circle distance metric. In this paper, we advance the understanding of covariance functions on spheres by defining the notion of and proving a characterization theorem for m times mean square differentiable processes on d-dimensional spheres. Stochastic processes on spheres are commonly constructed by restricting processes on Euclidean spaces to spheres of lower dimension. We prove that the resulting sphere-restricted process retains its differentiability properties, which has the important implication that the Matérn family retains its full range of smoothness when applied to spheres so long as Euclidean distance is used. The restriction operation has been questioned for using Euclidean instead of great circle distance. To address this question, we construct several new covariance functions and compare them to the Matérn with Euclidean distance on the task of interpolating smooth and non-smooth datasets. The Matérn with Euclidean distance is not outperformed by the new covariance functions or the existing covariance functions, so we recommend using the Matérn with Euclidean distance due to the ease with which it can be computed.},
	language = {en},
	urldate = {2022-03-06},
	journal = {Journal of Multivariate Analysis},
	author = {Guinness, Joseph and Fuentes, Montserrat},
	month = jan,
	year = {2016},
	pages = {143--152},
	file = {ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/Y5PT66HP/S0047259X15002109.html:text/html},
}

@misc{gmbh_httpswwwklokantechcom_wgs_nodate,
	title = {{WGS} 84 - {WGS84} - {World} {Geodetic} {System} 1984, used in {GPS} - {EPSG}:4326},
	shorttitle = {{WGS} 84 - {WGS84} - {World} {Geodetic} {System} 1984, used in {GPS} - {EPSG}},
	url = {http://epsg.io},
	abstract = {EPSG:4326 Geodetic coordinate system for World.  Horizontal component of 3D system. Used by the GPS satellite navigation system and for NATO military geodetic surveying.},
	language = {en},
	urldate = {2022-03-27},
	author = {GmbH (https://www.klokantech.com/), Klokan Technologies},
	file = {Snapshot:/Users/b7064522/Zotero/storage/87DPCLDV/4326.html:text/html},
}

@incollection{lu_geodetic_2014,
	address = {Berlin, Heidelberg},
	title = {Geodetic {Datum} and {Geodetic} {Control} {Networks}},
	isbn = {978-3-642-41245-5},
	url = {https://doi.org/10.1007/978-3-642-41245-5_3},
	abstract = {To measure terrain, surface features, position coordinates, heights, and gravity values at points on the Earth’s surface, there need to be corresponding reference points or surfaces (also known as datum points or surfaces), namely geodetic datums, to which surveying and mapping results are referred. Geodetic datums consist chiefly of coordinate datums (including classical horizontal datums and three-dimensional coordinate datums), vertical datums, sounding datums, as well as gravity datums. Geodetic datums provide initial data for all kinds of surveying and mapping work and serve as the foundation for determining the geometric shape and spatial–temporal distribution of geospatial information. Again, it is geodetic datums that are referred to when the spatial positions of geographical features in the real world are expressed in the data space. The missions of constructing geodetic datums include determining and defining the coordinate system, height system, and gravity reference system, and establishing and maintaining the coordinate framework (horizontal and satellite geodetic control networks), elevation framework (vertical control network), and gravimetric framework (gravity control network). This chapter mainly discusses geodetic datums and the methods, principles, and plans for establishing geodetic control networks.},
	language = {en},
	urldate = {2022-03-27},
	booktitle = {Geodesy: {Introduction} to {Geodetic} {Datum} and {Geodetic} {Systems}},
	publisher = {Springer},
	author = {Lu, Zhiping and Qu, Yunying and Qiao, Shubo},
	editor = {Lu, Zhiping and Qu, Yunying and Qiao, Shubo},
	year = {2014},
	doi = {10.1007/978-3-642-41245-5_3},
	pages = {71--130},
	file = {Springer Full Text PDF:/Users/b7064522/Zotero/storage/QUHL9DMK/Lu et al. - 2014 - Geodetic Datum and Geodetic Control Networks.pdf:application/pdf},
}

@inproceedings{hore_image_2010,
	address = {Istanbul, Turkey},
	title = {Image {Quality} {Metrics}: {PSNR} vs. {SSIM}},
	isbn = {978-1-4244-7542-1},
	shorttitle = {Image {Quality} {Metrics}},
	url = {http://ieeexplore.ieee.org/document/5596999/},
	doi = {10.1109/ICPR.2010.579},
	abstract = {In this paper, we analyse two well-known objective image quality metrics, the peak-signal-to-noise ratio (PSNR) as well as the structural similarity index measure (SSIM), and we derive a simple mathematical relationship between them which works for various kinds of image degradations such as Gaussian blur, additive Gaussian white noise, jpeg and jpeg2000 compression. A series of tests realized on images extracted from the Kodak database gives a better understanding of the similarity and difference between the SSIM and the PSNR.},
	language = {en},
	urldate = {2022-03-28},
	booktitle = {2010 20th {International} {Conference} on {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Hore, Alain and Ziou, Djemel},
	month = aug,
	year = {2010},
	pages = {2366--2369},
	file = {Hore and Ziou - 2010 - Image Quality Metrics PSNR vs. SSIM.pdf:/Users/b7064522/Zotero/storage/CALYZP2F/Hore and Ziou - 2010 - Image Quality Metrics PSNR vs. SSIM.pdf:application/pdf},
}

@article{hu_stochastic_2022,
	title = {A stochastic locally diffusive model with neural network-based deformations for global sea surface temperature},
	volume = {11},
	issn = {2049-1573},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.431},
	doi = {10.1002/sta4.431},
	abstract = {In this work, we propose a new approach to model large, irregularly distributed spatio-temporal global data via a locally diffusive stochastic partial differential equation (SPDE). The proposed model assumes a local deformation of the SPDE with non-linear dependence on the covariates through a neural network. The proposed model can be fit in a computationally efficient manner using a triangulation over the sphere and sparsity of the precision matrix, as shown in an application with a large data set of simulated multi-decadal monthly sea surface temperature.},
	language = {en},
	number = {1},
	urldate = {2022-04-01},
	journal = {Stat},
	author = {Hu, Wenjing and Fuglstad, Geir-Arne and Castruccio, Stefano},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sta4.431},
	pages = {e431},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/TNNW2HFR/Hu et al. - 2022 - A stochastic locally diffusive model with neural n.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/M7M3WNGH/sta4.html:text/html},
}

@article{meraner_cloud_2020,
	title = {Cloud removal in {Sentinel}-2 imagery using a deep residual neural network and {SAR}-optical data fusion},
	volume = {166},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271620301398},
	doi = {10.1016/j.isprsjprs.2020.05.013},
	abstract = {Optical remote sensing imagery is at the core of many Earth observation activities. The regular, consistent and global-scale nature of the satellite data is exploited in many applications, such as cropland monitoring, climate change assessment, land-cover and land-use classification, and disaster assessment. However, one main problem severely affects the temporal and spatial availability of surface observations, namely cloud cover. The task of removing clouds from optical images has been subject of studies since decades. The advent of the Big Data era in satellite remote sensing opens new possibilities for tackling the problem using powerful data-driven deep learning methods. In this paper, a deep residual neural network architecture is designed to remove clouds from multispectral Sentinel-2 imagery. SAR-optical data fusion is used to exploit the synergistic properties of the two imaging systems to guide the image reconstruction. Additionally, a novel cloud-adaptive loss is proposed to maximize the retainment of original information. The network is trained and tested on a globally sampled dataset comprising real cloudy and cloud-free images. The proposed setup allows to remove even optically thick clouds by reconstructing an optical representation of the underlying land surface structure.},
	language = {en},
	urldate = {2022-04-01},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Meraner, Andrea and Ebel, Patrick and Zhu, Xiao Xiang and Schmitt, Michael},
	month = aug,
	year = {2020},
	pages = {333--346},
	file = {Accepted Version:/Users/b7064522/Zotero/storage/RLXMAUPI/Meraner et al. - 2020 - Cloud removal in Sentinel-2 imagery using a deep r.pdf:application/pdf;ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/EJVR66P8/S0924271620301398.html:text/html},
}

@article{shen_missing_2015,
	title = {Missing {Information} {Reconstruction} of {Remote} {Sensing} {Data}: {A} {Technical} {Review}},
	volume = {3},
	issn = {2168-6831},
	shorttitle = {Missing {Information} {Reconstruction} of {Remote} {Sensing} {Data}},
	doi = {10.1109/MGRS.2015.2441912},
	abstract = {Because of sensor malfunction and poor atmospheric conditions, there is usually a great deal of missing information in optical remote sensing data, which reduces the usage rate and hinders the follow-up interpretation. In the past decades, missing information reconstruction of remote sensing data has become an active research field, and a large number of algorithms have been developed. However, to the best of our knowledge, there has not, to date, been a study that has been aimed at expatiating and summarizing the current situation. This is therefore our motivation in this review. This paper provides an introduction to the principles and theories of missing information reconstruction of remote sensing data. We classify the established and emerging algorithms into four main categories, followed by a comprehensive comparison of them from both experimental and theoretical perspectives. This paper also predicts the promising future research directions.},
	number = {3},
	journal = {IEEE Geoscience and Remote Sensing Magazine},
	author = {Shen, Huanfeng and Li, Xinghua and Cheng, Qing and Zeng, Chao and Yang, Gang and Li, Huifang and Zhang, Liangpei},
	month = sep,
	year = {2015},
	note = {Conference Name: IEEE Geoscience and Remote Sensing Magazine},
	pages = {61--85},
	file = {IEEE Xplore Abstract Record:/Users/b7064522/Zotero/storage/7SHDVEDU/7284768.html:text/html},
}

@book{wood_generalized_2006-1,
	address = {New York},
	title = {Generalized {Additive} {Models}: {An} {Introduction} with {R}},
	isbn = {978-0-429-09315-9},
	shorttitle = {Generalized {Additive} {Models}},
	abstract = {Now in widespread use, generalized additive models (GAMs) have evolved into a standard statistical methodology of considerable flexibility. While Hastie and Tibshirani's outstanding 1990 research monograph on GAMs is largely responsible for this, there has been a long-standing need for an accessible introductory treatment of the subject that also e},
	publisher = {Chapman and Hall/CRC},
	author = {Wood, Simon N.},
	month = feb,
	year = {2006},
	doi = {10.1201/9781420010404},
}

@article{wood_fast_2011,
	title = {Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models},
	volume = {73},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2010.00749.x},
	doi = {10.1111/j.1467-9868.2010.00749.x},
	abstract = {Summary. Recent work by Reiss and Ogden provides a theoretical basis for sometimes preferring restricted maximum likelihood (REML) to generalized cross-validation (GCV) for smoothing parameter selection in semiparametric regression. However, existing REML or marginal likelihood (ML) based methods for semiparametric generalized linear models (GLMs) use iterative REML or ML estimation of the smoothing parameters of working linear approximations to the GLM. Such indirect schemes need not converge and fail to do so in a non-negligible proportion of practical analyses. By contrast, very reliable prediction error criteria smoothing parameter selection methods are available, based on direct optimization of GCV, or related criteria, for the GLM itself. Since such methods directly optimize properly defined functions of the smoothing parameters, they have much more reliable convergence properties. The paper develops the first such method for REML or ML estimation of smoothing parameters. A Laplace approximation is used to obtain an approximate REML or ML for any GLM, which is suitable for efficient direct optimization. This REML or ML criterion requires that Newton–Raphson iteration, rather than Fisher scoring, be used for GLM fitting, and a computationally stable approach to this is proposed. The REML or ML criterion itself is optimized by a Newton method, with the derivatives required obtained by a mixture of implicit differentiation and direct methods. The method will cope with numerical rank deficiency in the fitted model and in fact provides a slight improvement in numerical robustness on the earlier method of Wood for prediction error criteria based smoothness selection. Simulation results suggest that the new REML and ML methods offer some improvement in mean-square error performance relative to GCV or Akaike's information criterion in most cases, without the small number of severe undersmoothing failures to which Akaike's information criterion and GCV are prone. This is achieved at the same computational cost as GCV or Akaike's information criterion. The new approach also eliminates the convergence failures of previous REML- or ML-based approaches for penalized GLMs and usually has lower computational cost than these alternatives. Example applications are presented in adaptive smoothing, scalar on function regression and generalized additive model selection.},
	language = {en},
	number = {1},
	urldate = {2022-04-04},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Wood, Simon N.},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2010.00749.x},
	pages = {3--36},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/Q7YJDS8M/Wood - 2011 - Fast stable restricted maximum likelihood and marg.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/C88APGAW/j.1467-9868.2010.00749.html:text/html},
}

@article{wood_fast_2008,
	title = {Fast stable direct fitting and smoothness selection for {Generalized} {Additive} {Models}},
	volume = {70},
	issn = {1369-7412, 1467-9868},
	url = {http://arxiv.org/abs/0709.3906},
	doi = {10.1111/j.1467-9868.2007.00646.x},
	abstract = {Existing computationally efficient methods for penalized likelihood GAM fitting employ iterative smoothness selection on working linear models (or working mixed models). Such schemes fail to converge for a non-negligible proportion of models, with failure being particularly frequent in the presence of concurvity. If smoothness selection is performed by optimizing `whole model' criteria these problems disappear, but until now attempts to do this have employed finite difference based optimization schemes which are computationally inefficient, and can suffer from false convergence. This paper develops the first computationally efficient method for direct GAM smoothness selection. It is highly stable, but by careful structuring achieves a computational efficiency that leads, in simulations, to lower mean computation times than the schemes based on working-model smoothness selection. The method also offers a reliable way of fitting generalized additive mixed models.},
	number = {3},
	urldate = {2022-04-04},
	journal = {J Royal Statistical Soc B},
	author = {Wood, Simon N.},
	month = jul,
	year = {2008},
	note = {arXiv: 0709.3906},
	pages = {495--518},
	file = {arXiv Fulltext PDF:/Users/b7064522/Zotero/storage/QFXIPAA3/Wood - 2008 - Fast stable direct fitting and smoothness selectio.pdf:application/pdf;arXiv.org Snapshot:/Users/b7064522/Zotero/storage/YFV7ZI3W/0709.html:text/html},
}

@article{dozat_incorporating_2016,
	title = {Incorporating {Nesterov} {Momentum} into {Adam}},
	url = {https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ},
	abstract = {This work aims to improve upon the recently proposed and rapidly popular-
ized optimization algorithm Adam (Kingma \& Ba, 2014). Adam has two main
components—a momentum component and an adaptive...},
	language = {en},
	urldate = {2022-04-04},
	author = {Dozat, Timothy},
	month = feb,
	year = {2016},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/H3RW6C9Y/Dozat - 2016 - Incorporating Nesterov Momentum into Adam.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/87WI522S/forum.html:text/html},
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2022-04-04},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	file = {arXiv Fulltext PDF:/Users/b7064522/Zotero/storage/TULGHI96/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/b7064522/Zotero/storage/UPBXPITA/1412.html:text/html},
}

@incollection{jolliffe_choosing_2002,
	address = {New York, NY},
	title = {Choosing a {Subset} of {Principal} {Components} or {Variables}},
	isbn = {978-0-387-22440-4},
	url = {https://doi.org/10.1007/0-387-22440-8_6},
	language = {en},
	urldate = {2022-04-05},
	booktitle = {Principal {Component} {Analysis}},
	publisher = {Springer},
	author = {Jolliffe, I. T.},
	year = {2002},
	doi = {10.1007/0-387-22440-8_6},
	pages = {111--149},
	file = {Springer Full Text PDF:/Users/b7064522/Zotero/storage/2ZYW5PDI/Jolliffe - 2002 - Choosing a Subset of Principal Components or Varia.pdf:application/pdf},
}

@article{josse_selecting_2012,
	title = {Selecting the number of components in principal component analysis using cross-validation approximations},
	volume = {56},
	issn = {0167-9473},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947311004099},
	doi = {10.1016/j.csda.2011.11.012},
	abstract = {Cross-validation is a tried and tested approach to select the number of components in principal component analysis (PCA), however, its main drawback is its computational cost. In a regression (or in a non parametric regression) setting, criteria such as the general cross-validation one (GCV) provide convenient approximations to leave-one-out cross-validation. They are based on the relation between the prediction error and the residual sum of squares weighted by elements of a projection matrix (or a smoothing matrix). Such a relation is then established in PCA using an original presentation of PCA with a unique projection matrix. It enables the definition of two cross-validation approximation criteria: the smoothing approximation of the cross-validation criterion (SACV) and the GCV criterion. The method is assessed with simulations and gives promising results.},
	language = {en},
	number = {6},
	urldate = {2022-04-05},
	journal = {Computational Statistics \& Data Analysis},
	author = {Josse, Julie and Husson, François},
	month = jun,
	year = {2012},
	pages = {1869--1879},
	file = {ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/S56YR83E/S0167947311004099.html:text/html},
}

@book{woodbury_inverting_1950,
	title = {Inverting {Modified} {Matrices}},
	language = {en},
	publisher = {Statistical Research Group},
	author = {Woodbury, Max A.},
	year = {1950},
	note = {Google-Books-ID: \_zAnzgEACAAJ},
}

@incollection{harville_determinants_1997,
	address = {New York, NY},
	title = {Determinants},
	isbn = {978-0-387-22677-4},
	url = {https://doi.org/10.1007/0-387-22677-X_13},
	abstract = {Determinants are encountered with considerable frequency in the statistics literature (and in the literature of various other disciplines that involve the notion of randomness). A determinant appears in the “normalizing constant” of the probability density function of the all-important multivariate normal distribution (e.g., Searle 1971, sec. 2.4f). And the definition of the generalized variance (or generalized dispersion) of a random vector involves a determinant—in the design of experiments, D-optimal designs are obtained by minimizing a generalized variance (e.g., Fedorov 1972).},
	language = {en},
	urldate = {2022-04-13},
	booktitle = {Matrix {Algebra} {From} a {Statistician}’s {Perspective}},
	publisher = {Springer},
	author = {Harville, David A.},
	editor = {Harville, David A.},
	year = {1997},
	doi = {10.1007/0-387-22677-X_13},
	pages = {179--208},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/Z9HNWSKT/Harville - 1997 - Determinants.pdf:application/pdf},
}

@book{sra_optimization_2012,
	title = {Optimization for {Machine} {Learning}},
	isbn = {978-0-262-01646-9},
	abstract = {An up-to-date account of the interplay between optimization and machine learning, accessible to students and researchers in both communities. The interplay between optimization and machine learning is one of the most important developments in modern computational science. Optimization formulations and methods are proving to be vital in designing algorithms to extract essential knowledge from huge volumes of data. Machine learning, however, is not simply a consumer of optimization technology but a rapidly evolving field that is itself generating new optimization ideas. This book captures the state of the art of the interaction between optimization and machine learning in a way that is accessible to researchers in both fields. Optimization approaches have enjoyed prominence in machine learning because of their wide applicability and attractive theoretical properties. The increasing complexity, size, and variety of today's machine learning models call for the reassessment of existing assumptions. This book starts the process of reassessment. It describes the resurgence in novel contexts of established frameworks such as first-order methods, stochastic approximations, convex relaxations, interior-point methods, and proximal methods. It also devotes attention to newer themes such as regularized optimization, robust optimization, gradient and subgradient methods, splitting techniques, and second-order methods. Many of these techniques draw inspiration from other fields, including operations research, theoretical computer science, and subfields of optimization. The book will enrich the ongoing cross-fertilization between the machine learning community and these other fields, and within the broader optimization community.},
	language = {en},
	publisher = {MIT Press},
	author = {Sra, Suvrit and Nowozin, Sebastian and Wright, Stephen J.},
	year = {2012},
	note = {Google-Books-ID: JPQx7s2L1A8C},
}

@inproceedings{li_efficient_2014,
	address = {New York New York USA},
	title = {Efficient mini-batch training for stochastic optimization},
	isbn = {978-1-4503-2956-9},
	url = {https://dl.acm.org/doi/10.1145/2623330.2623612},
	doi = {10.1145/2623330.2623612},
	abstract = {Stochastic gradient descent (SGD) is a popular technique for large-scale optimization problems in machine learning. In order to parallelize SGD, minibatch training needs to be employed to reduce the communication cost. However, an increase in minibatch size typically decreases the rate of convergence. This paper introduces a technique based on approximate optimization of a conservatively regularized objective function within each minibatch. We prove that the convergence rate does not decrease with increasing minibatch size. Experiments demonstrate that with suitable implementations of approximate optimization, the resulting algorithm can outperform standard SGD in many scenarios.},
	language = {en},
	urldate = {2022-04-15},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {ACM},
	author = {Li, Mu and Zhang, Tong and Chen, Yuqiang and Smola, Alexander J.},
	month = aug,
	year = {2014},
	pages = {661--670},
	file = {Li et al. - 2014 - Efficient mini-batch training for stochastic optim.pdf:/Users/b7064522/Zotero/storage/7KRNTBY7/Li et al. - 2014 - Efficient mini-batch training for stochastic optim.pdf:application/pdf},
}

@article{baydin_automatic_2018,
	title = {Automatic {Differentiation} in {Machine} {Learning}: a {Survey}},
	volume = {18},
	issn = {1533-7928},
	shorttitle = {Automatic {Differentiation} in {Machine} {Learning}},
	url = {http://jmlr.org/papers/v18/17-468.html},
	abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply âautodiffâ, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names âdynamic computational graphsâ and âdifferentiable programmingâ. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms âautodiffâ, âautomatic differentiationâ, and âsymbolic differentiationâ as these are encountered more and more in machine learning settings.},
	number = {153},
	urldate = {2022-04-19},
	journal = {Journal of Machine Learning Research},
	author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
	year = {2018},
	pages = {1--43},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/5FTZHDK5/Baydin et al. - 2018 - Automatic Differentiation in Machine Learning a S.pdf:application/pdf},
}

@article{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataﬂow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataﬂow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives ﬂexibility to the application developer: whereas in previous “parameter server” designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataﬂow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
	language = {en},
	author = {Abadi, Martın and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = nov,
	year = {2016},
	pages = {21},
	file = {Abadi et al. - TensorFlow A system for large-scale machine learn.pdf:/Users/b7064522/Zotero/storage/CH4BVK5K/Abadi et al. - TensorFlow A system for large-scale machine learn.pdf:application/pdf},
}

@article{neidinger_introduction_2010,
	title = {Introduction to {Automatic} {Differentiation} and {MATLAB} {Object}-{Oriented} {Programming}},
	volume = {52},
	issn = {0036-1445, 1095-7200},
	url = {http://epubs.siam.org/doi/10.1137/080743627},
	doi = {10.1137/080743627},
	abstract = {An introduction to both automatic diﬀerentiation and object-oriented programming can enrich a numerical analysis course that typically incorporates numerical diﬀerentiation and basic MATLAB computation. Automatic diﬀerentiation consists of exact algorithms on ﬂoating-point arguments. This implementation overloads standard elementary operators and functions in MATLAB with a derivative rule in addition to the function value; for example, sin u will also compute (cos u) ∗ u , where u and u are numerical values. These methods are mostly one-line programs that operate on a class of value-and-derivative objects, providing a simple example of object-oriented programming in MATLAB using the new (as of release 2008a) class deﬁnition structure. The resulting powerful tool computes derivative values and multivariable gradients, and is applied to Newton’s method for rootﬁnding in both single and multivariable settings. To compute higher-order derivatives of a single-variable function, another class of series objects keeps Taylor polynomial coeﬃcients up to some order. Overloading multiplication on series objects is a combination (discrete convolution) of coeﬃcients. This idea leads to algorithms for other operations and functions on series objects. A survey of more advanced topics in automatic diﬀerentiation includes an introduction to the reverse mode (our implementation is forward mode) and considerations in arbitrary-order multivariable series computation.},
	language = {en},
	number = {3},
	urldate = {2022-04-19},
	journal = {SIAM Rev.},
	author = {Neidinger, Richard D.},
	month = jan,
	year = {2010},
	pages = {545--563},
	file = {Neidinger - 2010 - Introduction to Automatic Differentiation and MATL.pdf:/Users/b7064522/Zotero/storage/FRG2EMLE/Neidinger - 2010 - Introduction to Automatic Differentiation and MATL.pdf:application/pdf},
}

@article{genton_classes_2001,
	title = {Classes of {Kernels} for {Machine} {Learning}: {A} {Statistics} {Perspective}},
	volume = {2},
	issn = {ISSN 1533-7928},
	shorttitle = {Classes of {Kernels} for {Machine} {Learning}},
	url = {https://www.jmlr.org/papers/v2/genton01a},
	abstract = {In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.},
	number = {Dec},
	urldate = {2022-05-01},
	journal = {Journal of Machine Learning Research},
	author = {Genton, Marc G.},
	year = {2001},
	pages = {299--312},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/JI4DWXBT/Genton - 2001 - Classes of Kernels for Machine Learning A Statist.pdf:application/pdf},
}

@book{hastie_elements_2009,
	address = {New York, NY},
	edition = {2nd ed},
	series = {Springer series in statistics},
	title = {The elements of statistical learning: data mining, inference, and prediction},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	shorttitle = {The elements of statistical learning},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
	year = {2009},
}
