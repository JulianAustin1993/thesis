
@article{deb_spatio-temporal_2017,
	title = {Spatio-temporal models with space-time interaction and their applications to air pollution data},
	url = {http://arxiv.org/abs/1801.00211},
	abstract = {It is of utmost importance to have a clear understanding of the status of air pollution and to provide forecasts and insights about the air quality to the general public and researchers in environmental studies. Previous studies of spatio-temporal models showed that even a shortterm exposure to high concentrations of atmospheric ﬁne particulate matters can be hazardous to the health of ordinary people. In this study, we develop a spatio-temporal model with space-time interaction for air pollution data (PM2.5). The proposed model uses a parametric space-time interaction component along with the spatial and temporal components in the mean structure, and introduces a random-eﬀects component speciﬁed in the form of zeromean spatio-temporal processes. For application, we analyze the air pollution data (PM2.5) from 66 monitoring stations across Taiwan.},
	language = {en},
	urldate = {2018-06-22},
	journal = {arXiv:1801.00211 [stat]},
	author = {Deb, Soudeep and Tsay, Ruey S.},
	month = dec,
	year = {2017},
	note = {arXiv: 1801.00211},
	keywords = {Separable Covariance Models}
}

@book{cressie_statistics_2011,
	address = {Hoboken, N.J},
	series = {Wiley series in probability and statistics},
	title = {Statistics for spatio-temporal data},
	isbn = {978-0-471-69274-4},
	number = {1},
	publisher = {Wiley},
	author = {Cressie, Noel A. C. and Wikle, Christopher K.},
	year = {2011},
	keywords = {Ordinary Kriging, Simple Kriging, Universal Kriging}
}

@article{cressie_classes_1999,
	title = {Classes of {Nonseparable}, {Spatio}-temporal {Stationary} {Covariance} {Functions}},
	volume = {94},
	abstract = {this article, we derive a new approach that allows one to obtain many classes of nonseparable, spatio-temporal stationary covariance functions and we fit several such to spatio-temporal data on wind speed over a region in the tropical western Pacific ocean. 1. INTRODUCTION},
	journal = {Journal of the American Statistical Association},
	author = {Cressie, Noel and Huang, Hsin-cheng},
	year = {1999},
	keywords = {Non-Separable Covariance Models},
	pages = {1330--1340}
}

@article{iaco_nonseparable_2002,
	title = {Nonseparable {Space}-{Time} {Covariance} {Models}: {Some} {Parametric} {Families}},
	volume = {34},
	issn = {0882-8121, 1573-8868},
	shorttitle = {Nonseparable {Space}-{Time} {Covariance} {Models}},
	url = {https://link.springer.com/article/10.1023/A:1014075310344},
	doi = {10.1023/A:1014075310344},
	abstract = {By extending the product and product–sum space-time covariance models, new families are generated as integrated products and product–sums. These include nonintegrable space-time covariance models not obtainable by the Cressie–Huang representation. It is shown how to fit the spatial and temporal components of the models as well as the probability density function. The methods are illustrated by a case study.},
	language = {en},
	number = {1},
	urldate = {2018-06-22},
	journal = {Mathematical Geology},
	author = {Iaco, S. De and Myers, D. E. and Posa, D.},
	month = jan,
	year = {2002},
	keywords = {Mixed Models, Non-Separable Covariance Models},
	pages = {23--42}
}

@article{gneiting_nonseparable_2002,
	title = {Nonseparable, {Stationary} {Covariance} {Functions} for {Space}-{Time} {Data}},
	volume = {97},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/3085674},
	abstract = {Geostatistical approaches to spatiotemporal prediction in environmental science, climatology, meteorology, and related fields rely on appropriate covariance models. This article proposes general classes of nonseparable, stationary covariance functions for spatiotemporal random processes. The constructions are directly in the space-time domain and do not depend on closed-form Fourier inversions. The model parameters can be associated with the data's spatial and temporal structures, respectively; and a covariance model with a readily interpretable space-time interaction parameter is fitted to wind data from Ireland.},
	number = {458},
	urldate = {2018-06-27},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann},
	year = {2002},
	keywords = {Non-Separable Covariance Models},
	pages = {590--600}
}

@article{aston_tests_2017,
	title = {Tests for separability in nonparametric covariance operators of random surfaces},
	volume = {45},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1498636862},
	doi = {10.1214/16-AOS1495},
	abstract = {The assumption of separability of the covariance operator for a random image or hypersurface can be of substantial use in applications, especially in situations where the accurate estimation of the full covariance structure is unfeasible, either for computational reasons, or due to a small sample size. However, inferential tools to verify this assumption are somewhat lacking in high-dimensional or functional data analysis settings, where this assumption is most relevant. We propose here to test separability by focusing on KKK-dimensional projections of the difference between the covariance operator and a nonparametric separable approximation. The subspace we project onto is one generated by the eigenfunctions of the covariance operator estimated under the separability hypothesis, negating the need to ever estimate the full nonseparable covariance. We show that the rescaled difference of the sample covariance operator with its separable approximation is asymptotically Gaussian. As a by-product of this result, we derive asymptotically pivotal tests under Gaussian assumptions, and propose bootstrap methods for approximating the distribution of the test statistics. We probe the finite sample performance through simulations studies, and present an application to log-spectrogram images from a phonetic linguistics dataset.},
	language = {EN},
	number = {4},
	urldate = {2018-07-20},
	journal = {The Annals of Statistics},
	author = {Aston, John A. D. and Pigoli, Davide and Tavakoli, Shahin},
	month = aug,
	year = {2017},
	mrnumber = {MR3670184},
	zmnumber = {06773279},
	pages = {1431--1461},
	file = {arXiv Fulltext PDF:/Users/b7064522/Zotero/storage/UE2KUHGT/Aston et al. - 2017 - Tests for separability in nonparametric covariance.pdf:application/pdf;arXiv.org Snapshot:/Users/b7064522/Zotero/storage/JCNHJMU4/1505.html:text/html;Full Text PDF:/Users/b7064522/Zotero/storage/LTL6DWMJ/Aston et al. - 2017 - Tests for separability in nonparametric covariance.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/NN3LT4NM/tests-for-separability-in-nonparametric-covariance-operators-of-random-surfaces(bf9cd29f-0336-4.html:text/html}
}

@book{ramsay_functional_2010,
	address = {New York (N.Y.)},
	title = {Functional data analysis},
	isbn = {978-1-4419-2300-4},
	language = {English},
	publisher = {Springer Science+Business Media},
	author = {Ramsay, James O and Silverman, Bernard W},
	year = {2010}
}

@article{mitchell_likelihood_2006,
	title = {A likelihood ratio test for separability of covariances},
	volume = {97},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X0500120X},
	doi = {10.1016/j.jmva.2005.07.005},
	abstract = {We propose a formal test of separability of covariance models based on a likelihood ratio statistic. The test is developed in the context of multivariate repeated measures (for example, several variables measured at multiple times on many subjects), but can also apply to a replicated spatio-temporal process and to problems in meteorology, where horizontal and vertical covariances are often assumed to be separable. Separable models are a common way to model spatio-temporal covariances because of the computational benefits resulting from the joint space–time covariance being factored into the product of a covariance function that depends only on space and a covariance function that depends only on time. We show that when the null hypothesis of separability holds, the distribution of the test statistic does not depend on the type of separable model. Thus, it is possible to develop reference distributions of the test statistic under the null hypothesis. These distributions are used to evaluate the power of the test for certain nonseparable models. The test does not require second-order stationarity, isotropy, or specification of a covariance model. We apply the test to a multivariate repeated measures problem.},
	number = {5},
	urldate = {2018-08-07},
	journal = {Journal of Multivariate Analysis},
	author = {Mitchell, Matthew W. and Genton, Marc G. and Gumpertz, Marcia L.},
	month = may,
	year = {2006},
	pages = {1025--1043}
}

@article{fuentes_testing_2006,
	title = {Testing for separability of spatial–temporal covariance functions},
	volume = {136},
	issn = {0378-3758},
	url = {http://www.sciencedirect.com/science/article/pii/S0378375804003210},
	doi = {10.1016/j.jspi.2004.07.004},
	abstract = {Most applications in spatial statistics involve modeling of complex spatial–temporal dependency structures, and many of the problems of space and time modeling can be overcome by using separable processes. This subclass of spatial–temporal processes has several advantages, including rapid fitting and simple extensions of many techniques developed and successfully used in time series and classical geostatistics. In particular, a major advantage of these processes is that the covariance matrix for a realization can be expressed as the Kronecker product of two smaller matrices that arise separately from the temporal and purely spatial processes, and hence its determinant and inverse are easily determinable. However, these separable models are not always realistic, and there are no formal tests for separability of general spatial–temporal processes. We present here a formal method to test for separability. Our approach can be also used to test for lack of stationarity of the process. The beauty of our approach is that by using spectral methods the mechanics of the test can be reduced to a simple two-factor analysis of variance (ANOVA) procedure. The approach we propose is based on only one realization of the spatial–temporal process. We apply the statistical methods proposed here to test for separability and stationarity of spatial–temporal ozone fields using data provided by the US Environmental Protection Agency (EPA).},
	number = {2},
	urldate = {2018-08-07},
	journal = {Journal of Statistical Planning and Inference},
	author = {Fuentes, Montserrat},
	month = feb,
	year = {2006},
	pages = {447--466}
}

@book{oliver_understanding_2004,
	address = {Raleigh, NC},
	title = {Understanding {Synthetic} {Aperture} {Radar} {Images}},
	isbn = {978-1-891121-31-9},
	abstract = {Written for SAR system designers and remote sensing specialists, this practical reference shows you how to produce higher quality SAR images using data-driven algorithms, and how to apply powerful new techniques to measure and analyze SAR image content.The book describes how SAR imagery is formed, how SAR processing affects image properties, and gives you specific guidance in selecting and applying today's most sophisticated analytical techniques. By helping you to quickly assess which components of an SAR image should be measured, the book enables you to devote more time and energy to the real task of image interpretation and analysis.Now includes a CD-ROM featuring a 2-month free license to InfoPACK Version 1.2! InfoPACK, developed by Chris Oliver, is a SAR image interpretation software suite which exploits and extends the principles described in the book, enabling the user to run many of the algorithms. It features an easy to use GUI, image viewer and versatile scripting language, making applications development easy and fast. Particular aspects of interest include segmentation, classification (supervised and unsupervised), speckle reduction and a host of techniques for data fusion. Routines within InfoPACK include:* Intensity Segmentation (single image; multitemporal, multipolarization)* Speckle Reduction* Texture Segmentation* Segmentation Postprocessing* Classification* Edge Detection * Point Target Detection* Large Area Change DetectionAlso included are a variety of low-level filters and functions for image manipulation and arithmetic operations.Key Features of the Book* Filled with real-world examples from various SAR systems, the book reveals sophisticated, proven techniques for:* SAR filtering, parameter estimation, image reconstruction, segmentation, and classification * Evaluating measurement algorithms to help make better algorithm selections * Applying powerful speckle removal methods to airborne and spaceborne SAR data},
	language = {English},
	publisher = {Scitech Publishing},
	author = {Oliver, Christopher},
	month = dec,
	year = {2004}
}

@incollection{militino_introduction_2018,
	address = {Cham},
	title = {An {Introduction} to the {Spatio}-{Temporal} {Analysis} of {Satellite} {Remote} {Sensing} {Data} for {Geostatisticians}},
	isbn = {978-3-319-78999-6},
	url = {https://doi.org/10.1007/978-3-319-78999-6_13},
	abstract = {Satellite remote sensing data have become available in meteorology, agriculture, forestry, geology, regional planning, hydrology or natural environment sciences since several decades ago, because satellites provide routinely high quality images with different temporal and spatial resolutions. Joining, combining or smoothing these images for a better quality of information is a challenge not always properly solved. In this regard, geostatistics, as the spatio-temporal stochastic techniques of geo-referenced data, is a very helpful and powerful tool not enough explored in this area yet. Here, we analyze the current use of some of the geostatistical tools in satellite image analysis, and provide an introduction to this subject for potential researchers.},
	language = {en},
	urldate = {2019-06-02},
	booktitle = {Handbook of {Mathematical} {Geosciences}: {Fifty} {Years} of {IAMG}},
	publisher = {Springer International Publishing},
	author = {Militino, A. F. and Ugarte, M. D. and Pérez-Goya, U.},
	editor = {Daya Sagar, B.S. and Cheng, Qiuming and Agterberg, Frits},
	year = {2018},
	doi = {10.1007/978-3-319-78999-6_13},
	pages = {239--253},
	file = {Springer Full Text PDF:/Users/b7064522/Zotero/storage/SCY6D7AG/Militino et al. - 2018 - An Introduction to the Spatio-Temporal Analysis of.pdf:application/pdf}
}

@article{aschbacher_european_2012,
	series = {The {Sentinel} {Missions} - {New} {Opportunities} for {Science}},
	title = {The {European} {Earth} monitoring ({GMES}) programme: {Status} and perspectives},
	volume = {120},
	issn = {0034-4257},
	shorttitle = {The {European} {Earth} monitoring ({GMES}) programme},
	url = {http://www.sciencedirect.com/science/article/pii/S0034425712000612},
	doi = {10.1016/j.rse.2011.08.028},
	abstract = {Global Monitoring for Environment and Security (GMES) is the most ambitious operational Earth Observation programme to date and will provide global, timely and easily accessible information in application domains such as land, marine, atmosphere, emergency response, climate change and security. To accomplish this, the European Union (EU)-led GMES programme comprises three components namely the space component, the in-situ component and the service component. The space component, led by ESA, is in its pre-operational stage, serving users with satellite data acquired by the so called “GMES Contributing Missions” already available today or planned at European, national and international level. It will become operational once the dedicated space infrastructure, comprised by the “Sentinel” missions and their corresponding ground segments are operational. The first of these satellite series will be launched in 2013. The Sentinel missions will provide a unique set of observations utilising different techniques spanning C-band SAR, mid to medium resolution optical and thermal observations with increased spectral resolution, altimeter and dedicated spectrometers for atmospheric chemistry. This data, combined with in-situ data, some assimilated into models, will then be turned into services for monitoring the environment, the climate and for security related issues. The GMES Space Component (GSC) is organised in two overlapping phases: the development phase and the operational phase, the latter planned to start in 2014. The main challenge is now to ensure the programme's long-term sustainability.},
	urldate = {2019-06-17},
	journal = {Remote Sensing of Environment},
	author = {Aschbacher, Josef and Milagro-Pérez, Maria Pilar},
	month = may,
	year = {2012},
	pages = {3--8},
	file = {ScienceDirect Full Text PDF:/Users/b7064522/Zotero/storage/SMFLBCG2/Aschbacher and Milagro-Pérez - 2012 - The European Earth monitoring (GMES) programme St.pdf:application/pdf;ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/IVXY3JQM/S0034425712000612.html:text/html}
}

@article{george_selecting_2015,
	title = {Selecting a {Separable} {Parametric} {Spatiotemporal} {Covariance} {Structure} for {Longitudinal} {Imaging} {Data}},
	volume = {34},
	issn = {0277-6715},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4262538/},
	doi = {10.1002/sim.6324},
	abstract = {Longitudinal imaging studies allow great insight into how the structure and function of a subject’s internal anatomy changes over time. Unfortunately, the analysis of longitudinal imaging data is complicated by inherent spatial and temporal correlation: the temporal from the repeated measures, and the spatial from the outcomes of interest being observed at multiple points in a patients body. We propose the use of a linear model with a separable parametric spatiotemporal error structure for the analysis of repeated imaging data. The model makes use of spatial (exponential, spherical, and Matérn) and temporal (compound symmetric, autoregressive-1, Toeplitz, and unstructured) parametric correlation functions., A simulation study, inspired by a longitudinal cardiac imaging study on mitral regurgitation patients, compared different information criteria for selecting a particular separable parametric spatiotemporal correlation structure as well as the effects on Type I and II error rates for inference on fixed effects when the specified model is incorrect. Information criteria were found to be highly accurate at choosing between separable parametric spatiotemporal correlation structures. Misspecification of the covariance structure was found to have the ability to inflate the Type I error or have an overly conservative test size, which corresponded to decreased power., An example with clinical data is given illustrating how the covariance structure procedure can be done in practice, as well as how covariance structure choice can change inferences about fixed effects.},
	number = {1},
	urldate = {2019-06-24},
	journal = {Statistics in medicine},
	author = {George, Brandon and Aban, Inmaculada},
	month = jan,
	year = {2015},
	pmid = {25293361},
	pmcid = {PMC4262538},
	pages = {145--161},
	file = {PubMed Central Full Text PDF:/Users/b7064522/Zotero/storage/HXL239HQ/George and Aban - 2015 - Selecting a Separable Parametric Spatiotemporal Co.pdf:application/pdf}
}

@book{shi_gaussian_2011,
	title = {Gaussian {Process} {Regression} {Analysis} for {Functional} {Data}},
	isbn = {978-1-4398-3773-3},
	abstract = {Gaussian Process Regression Analysis for Functional Data presents nonparametric statistical methods for functional regression analysis, specifically the methods based on a Gaussian process prior in a functional space. The authors focus on problems involving functional response variables and mixed covariates of functional and scalar variables.  Covering the basics of Gaussian process regression, the first several chapters discuss functional data analysis, theoretical aspects based on the asymptotic properties of Gaussian process regression models, and new methodological developments for high dimensional data and variable selection. The remainder of the text explores advanced topics of functional regression analysis, including novel nonparametric statistical methods for curve prediction, curve clustering, functional ANOVA, and functional regression analysis of batch data, repeated curves, and non-Gaussian data.  Many flexible models based on Gaussian processes provide efficient ways of model learning, interpreting model structure, and carrying out inference, particularly when dealing with large dimensional functional data. This book shows how to use these Gaussian process regression models in the analysis of functional data. Some MATLAB® and C codes are available on the first author’s website.},
	language = {en},
	publisher = {CRC Press},
	author = {Shi, Jian Qing and Choi, Taeryon},
	month = jul,
	year = {2011},
	note = {Google-Books-ID: DkgdN6dRAicC}
}

@article{wood_p-splines_2017,
	title = {P-splines with derivative based penalties and tensor product smoothing of unevenly distributed data},
	volume = {27},
	issn = {0960-3174, 1573-1375},
	url = {http://arxiv.org/abs/1605.02446},
	doi = {10.1007/s11222-016-9666-x},
	abstract = {The P-splines of Eilers and Marx (1996) combine a B-spline basis with a discrete quadratic penalty on the basis coefﬁcients, to produce a reduced rank spline like smoother. P-splines have three properties that make them very popular as reduced rank smoothers: i) the basis and the penalty are sparse, enabling efﬁcient computation, especially for Bayesian stochastic simulation; ii) it is possible to ﬂexibly ‘mix-and-match’ the order of B-spline basis and penalty, rather than the order of penalty controlling the order of the basis as in spline smoothing; iii) it is very easy to set up the Bspline basis functions and penalties. The discrete penalties are somewhat less interpretable in terms of function shape than the traditional derivative based spline penalties, but tend towards penalties proportional to traditional spline penalties in the limit of large basis size. However part of the point of P-splines is not to use a large basis size. In addition the spline basis functions arise from solving functional optimization problems involving derivative based penalties, so moving to discrete penalties for smoothing may not always be desirable. The purpose of this note is to point out that the three properties of basis-penalty sparsity, mix-and-match penalization and ease of setup are readily obtainable with B-splines subject to derivative based penalization. The penalty setup typically requires a few lines of code, rather than the two lines typically required for P-splines, but this one off disadvantage seems to be the only one associated with using derivative based penalties. As an example application, it is shown how basis-penalty sparsity enables efﬁcient computation with tensor product smoothers of scattered data.},
	language = {en},
	number = {4},
	urldate = {2020-03-11},
	journal = {Statistics and Computing},
	author = {Wood, Simon N.},
	month = jul,
	year = {2017},
	note = {arXiv: 1605.02446},
	pages = {985--989}
}

@book{wahba_spline_1990,
	series = {{CBMS}-{NSF} {Regional} {Conference} {Series} in {Applied} {Mathematics}},
	title = {Spline {Models} for {Observational} {Data}},
	isbn = {978-0-89871-244-5},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611970128},
	abstract = {This monograph is based on a series of 10 lectures at Ohio State University at Columbus, March 23–27, 1987, sponsored by the Conference Board of the Mathematical Sciences and the National Science Foundation. The selection of topics is quite personal and, together with the talks of the other speakers, the lectures represent a story, as I saw it in March 1987, of many of the interesting things that statisticians can do with splines. I told the audience that the priority order for topic selection was, first, obscure work of my own and collaborators, second, other work by myself and students, with important work by other speakers deliberately omitted in the hope that they would mention it themselves. This monograph will more or less follow that outline, so that it is very much slanted toward work I had some hand in, although I will try to mention at least by reference important work by the other speakers and some of the attendees. The other speakers were (in alphabetical order), Dennis Cox, Randy Eubank, Ker-Chau Li, Douglas Nychka, David Scott, Bernard Silverman, Paul Speckman, and James Wendelberger. The work of Finbarr O'Sullivan, who was unable to attend, in extending the developing theory to the non-Gaussian and nonlinear case will also play a central role, as will the work of Florencio Utreras.},
	urldate = {2020-04-24},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Wahba, Grace},
	month = jan,
	year = {1990},
	doi = {10.1137/1.9781611970128},
	file = {Snapshot:/Users/b7064522/Zotero/storage/C8BVU8RL/1.html:text/html}
}

@book{hyndman_forecasting_2018,
	title = {Forecasting: principles and practice},
	isbn = {978-0-9875071-1-2},
	shorttitle = {Forecasting},
	abstract = {Forecasting is required in many situations. Stocking an inventory may require forecasts of demand months in advance. Telecommunication routing requires traffic forecasts a few minutes ahead. Whatever the circumstances or time horizons involved, forecasting is an important aid in effective and efficient planning.This textbook provides a comprehensive introduction to forecasting methods and presents enough information about each method for readers to use them sensibly.},
	language = {en},
	publisher = {OTexts},
	author = {Hyndman, Rob J. and Athanasopoulos, George},
	month = may,
	year = {2018},
	note = {Google-Books-ID: \_bBhDwAAQBAJ}
}

@article{muro_short-term_2016,
	title = {Short-{Term} {Change} {Detection} in {Wetlands} {Using} {Sentinel}-1 {Time} {Series}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/8/10/795},
	doi = {10.3390/rs8100795},
	abstract = {Automated monitoring systems that can capture wetlands’ high spatial and temporal variability are essential for their management. SAR-based change detection approaches offer a great opportunity to enhance our understanding of complex and dynamic ecosystems. We test a recently-developed time series change detection approach (S1-omnibus) using Sentinel-1 imagery of two wetlands with different ecological characteristics; a seasonal isolated wetland in southern Spain and a coastal wetland in the south of France. We test the S1-omnibus method against a commonly-used pairwise comparison of consecutive images to demonstrate its advantages. Additionally, we compare it with a pairwise change detection method using a subset of consecutive Landsat images for the same period of time. The results show how S1-omnibus is capable of capturing in space and time changes produced by water surface dynamics, as well as by agricultural practices, whether they are sudden changes, as well as gradual. S1-omnibus is capable of detecting a wider array of short-term changes than when using consecutive pairs of Sentinel-1 images. When compared to the Landsat-based change detection method, both show an overall good agreement, although certain landscape changes are detected only by either the Landsat-based or the S1-omnibus method. The S1-omnibus method shows a great potential for an automated monitoring of short time changes and accurate delineation of areas of high variability and of slow and gradual changes.},
	language = {en},
	number = {10},
	urldate = {2020-04-24},
	journal = {Remote Sensing},
	author = {Muro, Javier and Canty, Morton and Conradsen, Knut and Hüttich, Christian and Nielsen, Allan Aasbjerg and Skriver, Henning and Remy, Florian and Strauch, Adrian and Thonfeld, Frank and Menz, Gunter},
	month = oct,
	year = {2016},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {795},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/3LH6YJTT/Muro et al. - 2016 - Short-Term Change Detection in Wetlands Using Sent.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/WIIX4U8D/795.html:text/html}
}

@inproceedings{hooker_maximal_2015,
	title = {Maximal autocorrelation factors for function-valued spatial/temporal data},
	isbn = {978-0-9872143-5-5},
	url = {http://www.mssanz.org.au/modsim2015/A3/hooker.pdf},
	doi = {10.36334/MODSIM.2015.A3.Hooker},
	abstract = {Dimension reduction techniques play a key role in analyzing functional data that possess temporal or spatial dependence. Of these dimension reduction techniques functional principal components analysis (FPCA) remains a popular approach. Functional principal components extract a set of latent components by maximizing variance in a set of dependent functional data. However, this technique may fail to adequately capture temporal or spatial autocorrelation.},
	language = {en},
	urldate = {2020-05-01},
	booktitle = {Weber, {T}., {McPhee}, {M}.{J}. and {Anderssen}, {R}.{S}. (eds) {MODSIM2015}, 21st {International} {Congress} on {Modelling} and {Simulation}},
	publisher = {Modelling and Simulation Society of Australia and New Zealand},
	author = {Hooker, G and Roberts, S and Shang, Lin, Han},
	month = nov,
	year = {2015},
	file = {2015 - Maximal autocorrelation factors for function-value.pdf:/Users/b7064522/Zotero/storage/HYK7V3CD/2015 - Maximal autocorrelation factors for function-value.pdf:application/pdf}
}

@article{karhunen_zur_1946,
	title = {Zur {Spektraltheorie} stochastischer {Prozesse}},
	volume = {1},
	journal = {Ann. Acad. Sci. Finnicae, Ser. A},
	author = {Karhunen, K.},
	year = {1946},
	keywords = {imported},
	pages = {34}
}

@article{loeve_fonctions_1946,
	title = {Fonctions aléatoires à décomposition orthogonale exponentielle},
	volume = {84},
	journal = {La Revue Scientifique},
	author = {Loève, Michel},
	year = {1946},
	pages = {159--162}
}

@article{hyndman_automatic_2008,
	title = {Automatic time series forecasting: the forecast package for {R}},
	volume = {26},
	url = {http://www.jstatsoft.org/article/view/v027i03},
	number = {3},
	journal = {Journal of Statistical Software},
	author = {Hyndman, Rob J. and Khandakar, Yeasmin},
	year = {2008},
	pages = {1--22}
}

@book{de_boor_practical_2001,
	address = {New York},
	edition = {Rev. ed},
	series = {Applied mathematical sciences},
	title = {A practical guide to splines: with 32 figures},
	isbn = {978-0-387-95366-3},
	shorttitle = {A practical guide to splines},
	number = {v. 27},
	publisher = {Springer},
	author = {De Boor, Carl},
	year = {2001},
	file = {Snapshot:/Users/b7064522/Zotero/storage/M4AQHHVZ/Boor - 1978 - A Practical Guide to Splines.html:text/html;Snapshot:/Users/b7064522/Zotero/storage/XZK2GBMT/9780387953663.html:text/html}
}

@article{wold_principal_1987,
	title = {Principal component analysis},
	volume = {2},
	number = {1-3},
	journal = {Chemometrics and intelligent laboratory systems},
	author = {Wold, Svante and Esbensen, Kim and Geladi, Paul},
	year = {1987},
	note = {Publisher: Elsevier},
	pages = {37--52}
}

@article{yao_functional_2005,
	title = {Functional {Data} {Analysis} for {Sparse} {Longitudinal} {Data}},
	volume = {100},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/27590579},
	abstract = {We propose a nonparametric method to perform functional principal components analysis for the case of sparse longitudinal data. The method aims at irregularly spaced longitudinal data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject. We assume that the repeated measurements are located randomly with a random number of repetitions for each subject and are determined by an underlying smooth random (subject-specific) trajectory plus measurement errors. Basic elements of our approach are the parsimonious estimation of the covariance structure and mean function of the trajectories, and the estimation of the variance of the measurement errors. The eigenfunction basis is estimated from the data, and functional principal components score estimates are obtained by a conditioning step. This conditional estimation method is conceptually simple and straightforward to implement. A key step is the derivation of asymptotic consistency and distribution results under mild conditions, using tools from functional analysis. Functional data analysis for sparse longitudinal data enables prediction of individual smooth trajectories even if only one or few measurements are available for a subject. Asymptotic pointwise and simultaneous confidence bands are obtained for predicted individual trajectories, based on asymptotic distributions, for simultaneous bands under the assumption of a finite number of components. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. The methods are illustrated with a simulation study, longitudinal CD4 data for a sample of AIDS patients, and time-course gene expression data for the yeast cell cycle.},
	number = {470},
	urldate = {2020-05-14},
	journal = {Journal of the American Statistical Association},
	author = {Yao, Fang and Müller, Hans-Georg and Wang, Jane-Ling},
	year = {2005},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {577--590},
	file = {Yao et al. - 2005 - Functional Data Analysis for Sparse Longitudinal D.pdf:/Users/b7064522/Zotero/storage/SW96SQ9Y/Yao et al. - 2005 - Functional Data Analysis for Sparse Longitudinal D.pdf:application/pdf}
}

@book{williams_gaussian_2006,
	title = {Gaussian processes for machine learning},
	volume = {2},
	number = {3},
	publisher = {MIT press Cambridge, MA},
	author = {Williams, Christopher KI and Rasmussen, Carl Edward},
	year = {2006}
}

@article{hooker_maximal_2016,
	title = {Maximal autocorrelation functions in functional data analysis},
	volume = {26},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-015-9582-5},
	doi = {10.1007/s11222-015-9582-5},
	abstract = {This paper proposes a new factor rotation for the context of functional principal components analysis. This rotation seeks to re-express a functional subspace in terms of directions of decreasing smoothness as represented by a generalized smoothing metric. The rotation can be implemented simply and we show on two examples that this rotation can improve the interpretability of the leading components.},
	language = {en},
	number = {5},
	urldate = {2020-06-04},
	journal = {Statistics and Computing},
	author = {Hooker, Giles and Roberts, Steven},
	month = sep,
	year = {2016},
	pages = {945--950}
}

@article{liu_functional_2012,
	title = {Functional factor analysis for periodic remote sensing data},
	volume = {6},
	issn = {1932-6157},
	url = {http://projecteuclid.org/euclid.aoas/1339419609},
	doi = {10.1214/11-AOAS518},
	language = {en},
	number = {2},
	urldate = {2020-07-09},
	journal = {The Annals of Applied Statistics},
	author = {Liu, Chong and Ray, Surajit and Hooker, Giles and Friedl, Mark},
	month = jun,
	year = {2012},
	pages = {601--624},
	file = {Liu et al. - 2012 - Functional factor analysis for periodic remote sen.pdf:/Users/b7064522/Zotero/storage/GJUIBF4H/Liu et al. - 2012 - Functional factor analysis for periodic remote sen.pdf:application/pdf}
}

@article{khabbazan_crop_2019,
	title = {Crop {Monitoring} {Using} {Sentinel}-1 {Data}: {A} {Case} {Study} from {The} {Netherlands}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Crop {Monitoring} {Using} {Sentinel}-1 {Data}},
	url = {https://www.mdpi.com/2072-4292/11/16/1887},
	doi = {10.3390/rs11161887},
	abstract = {Agriculture is of huge economic significance in The Netherlands where the provision of real-time, reliable information on crop development is essential to support the transition towards precision agriculture. Optical imagery can provide invaluable insights into crop growth and development but is severely hampered by cloud cover. This case study in the Flevopolder illustrates the potential value of Sentinel-1 for monitoring five key crops in The Netherlands, namely sugar beet, potato, maize, wheat and English rye grass. Time series of radar backscatter from the European Space Agency’s Sentinel-1 Mission are analyzed and compared to ground measurements including phenological stage and height. Temporal variations in backscatter data reflect changes in water content and structure associated with phenological development. Emergence and closure dates are estimated from the backscatter time series and validated against a photo archive. Coherence data are compared to Normalized Difference Vegetation Index (NDVI) and ground data, illustrating that the sudden increase in coherence is a useful indicator of harvest. The results presented here demonstrate that Sentinel-1 data have significant potential value to monitor growth and development of key Dutch crops. Furthermore, the guaranteed availability of Sentinel-1 imagery in clouded conditions ensures the reliability of data to meet the monitoring needs of farmers, food producers and regulatory bodies.},
	language = {en},
	number = {16},
	urldate = {2020-11-04},
	journal = {Remote Sensing},
	author = {Khabbazan, Saeed and Vermunt, Paul and Steele-Dunne, Susan and Ratering Arntz, Lexy and Marinetti, Caterina and van der Valk, Dirk and Iannini, Lorenzo and Molijn, Ramses and Westerdijk, Kees and van der Sande, Corné},
	month = jan,
	year = {2019},
	note = {Number: 16
Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {1887},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/MR8ZQ8MT/Khabbazan et al. - 2019 - Crop Monitoring Using Sentinel-1 Data A Case Stud.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/FZKW24Y2/1887.html:text/html;Snapshot:/Users/b7064522/Zotero/storage/LRBI57WR/1887.html:text/html}
}

@article{raspini_continuous_2018,
	title = {Continuous, semi-automatic monitoring of ground deformation using {Sentinel}-1 satellites},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-25369-w},
	doi = {10.1038/s41598-018-25369-w},
	abstract = {We present the continuous monitoring of ground deformation at regional scale using ESA (European Space Agency) Sentinel-1constellation of satellites. We discuss this operational monitoring service through the case study of the Tuscany Region (Central Italy), selected due to its peculiar geological setting prone to ground instability phenomena. We set up a systematic processing chain of Sentinel-1 acquisitions to create continuously updated ground deformation data to mark the transition from static satellite analysis, based on the analysis of archive images, to dynamic monitoring of ground displacement. Displacement time series, systematically updated with the most recent available Sentinel-1 acquisition, are analysed to identify anomalous points (i.e., points where a change in the dynamic of motion is occurring). The presence of a cluster of persistent anomalies affecting elements at risk determines a significant level of risk, with the necessity of further analysis. Here, we show that the Sentinel-1 constellation can be used for continuous and systematic tracking of ground deformation phenomena at the regional scale. Our results demonstrate how satellite data, acquired with short revisiting times and promptly processed, can contribute to the detection of changes in ground deformation patterns and can act as a key information layer for risk mitigation.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Scientific Reports},
	author = {Raspini, Federico and Bianchini, Silvia and Ciampalini, Andrea and Del Soldato, Matteo and Solari, Lorenzo and Novali, Fabrizio and Del Conte, Sara and Rucci, Alessio and Ferretti, Alessandro and Casagli, Nicola},
	month = may,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {7253},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/P36EWZ9E/Raspini et al. - 2018 - Continuous, semi-automatic monitoring of ground de.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/WCPPKAQM/s41598-018-25369-w.html:text/html}
}

@book{cressie_statistics_2010,
	address = {New York},
	title = {Statistics for spatial data},
	isbn = {978-1-119-11515-1 978-0-471-00255-0},
	abstract = {The Wiley Classics Library consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists. Spatial statistics - analyzing spatial data through statistical models - has proven exceptionally versatile, encompassing problems ranging from the microscopic to the astronomic. However, for the scientist and engineer faced only with scattered and uneven treatments of the subject in the scientific literature, learning how to make practical use of spatial statistics in day-to-day analytical work is very difficult. Designed exclusively for scientists eager to tap into the enormous potential of this analytical tool and upgrade their range of technical skills, Statistics for Spatial Data is a comprehensive, single-source guide to both the theory and applied aspects of spatial statistical methods. The hard-cover edition was hailed by Mathematical Reviews as an "excellent book which will become a basic reference." This paper-back edition of the 1993 edition, is designed to meet the many technological challenges facing the scientist and engineer. Concentrating on the three areas of geostatistical data, lattice data, and point patterns, the book sheds light on the link between data and model, revealing how design, inference, and diagnostics are an outgrowth of that link. It then explores new methods to reveal just how spatial statistical models can be used to solve important problems in a host of areas in science and engineering. Discussion includes: exploratory spatial data analysis, spectral theory for stationary processes, spatial scale, simulation methods for spatial processes, spatial bootstrapping, statistical image analysis and remote sensing, computational aspects of model fitting, application of models to disease mapping. Designed to accommodate the practical needs of the professional, it features a unified and common notation for its subject as well as many detailed examples woven into the text, numerous illustrations (including graphs that illuminate the theory discussed) and over 1,000 references. Fully balancing theory with applications, Statistics for Spatial Data, Revised Edition is an exceptionally clear guide on making optimal use of one of the ascendant analytical tools of the decade, one that has begun to capture the imagination of professionals in biology, earth science, civil, electrical, and agricultural engineering, geography, epidemiology, and ecology.},
	language = {English},
	publisher = {Wiley},
	author = {Cressie, Noel A. C},
	year = {2010},
	note = {OCLC: 1039155476}
}

@book{stein_interpolation_1999,
	title = {Interpolation of {Spatial} {Data} {Some} {Theory} for {Kriging}},
	isbn = {978-1-4612-1494-6},
	url = {https://doi.org/10.1007/978-1-4612-1494-6},
	language = {German},
	urldate = {2020-11-04},
	author = {Stein, Michael L},
	year = {1999},
	note = {OCLC: 1184292315}
}

@article{rossi_kriging_1994,
	title = {Kriging in the shadows: {Geostatistical} interpolation for remote sensing},
	volume = {49},
	issn = {0034-4257},
	shorttitle = {Kriging in the shadows},
	url = {http://www.sciencedirect.com/science/article/pii/0034425794900574},
	doi = {10.1016/0034-4257(94)90057-4},
	abstract = {It is often useful to estimate obscured or missing remotely sensed data. Traditional interpolation methods, such as nearest-neighbor or bilinear resampling, do not take full advantage of the spatial information in the image. An alternative method, a geostatistical technique known as indicator kriging, is described and demonstrated using a Landsat Thematic Mapper image in southern Chiapas, Mexico. The image was first classified into pasture and nonpasture land cover. For each pixel that was obscured by cloud or cloud shadow, the probability that it was pasture was assigned by the algorithm. An exponential omnidirectional variogram model was used to characterize the spatial continuity of the image for use in the kriging algorithm. Assuming a cutoff probability level of 50\%, the error was shown to be 17\% with no obvious spatial bias but with some tendency to categorize nonpasture as pasture (overestimation). While this is a promising result, the method's practical application in other missing data problems for remotely sensed images will depend on the amount and spatial pattern of the unobscured pixels and missing pixels and the success of the spatial continuity model used.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {Remote Sensing of Environment},
	author = {Rossi, Richard E. and Dungan, Jennifer L. and Beck, Louisa R.},
	month = jul,
	year = {1994},
	pages = {32--40},
	file = {ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/CXVK2VRI/0034425794900574.html:text/html}
}

@article{zhang_restoration_2009,
	title = {Restoration of clouded pixels in multispectral remotely sensed imagery with cokriging},
	volume = {30},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431160802549294},
	doi = {10.1080/01431160802549294},
	abstract = {The presence of clouds and their shadows in remotely sensed images limits their potential uses for extracting information. The commonly used methods for replacing clouded pixels by land cover reflection estimates usually yield poor results if the images being combined exhibit radical differences in target radiance due, for example, to large date separation and high temporal variability. This study focuses on introducing geostatistical techniques for interpolating the DN values of clouded pixels in multispectral remotely sensed images using traditional ordinary cokriging and standardized ordinary cokriging. Two case studies were conducted in this study. The first case study shows that the methods work well for the small clouds in a heterogeneous landscape even when the images being combined show high temporal variability. Although the basic spatial structure in large size clouds can be captured, image interpolation‐related artefacts such as smoothing effects are visually apparent in a heterogeneous landscape. The second case study indicates that the cokriging methods work better in homogenous regions such as the dominantly agricultural areas in United States Midwest. Various statistics including both global statistics and local statistics are employed to confirm the reliability of the methods.},
	number = {9},
	urldate = {2020-11-04},
	journal = {International Journal of Remote Sensing},
	author = {Zhang, Chuanrong and Li, Weidong and Travis, David J.},
	month = may,
	year = {2009},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01431160802549294},
	pages = {2173--2195},
	file = {Snapshot:/Users/b7064522/Zotero/storage/K3KVL58K/01431160802549294.html:text/html}
}

@article{schmidt_flexible_2020,
	series = {Frontiers in {Spatial} and {Spatio}-temporal {Research}},
	title = {Flexible spatial covariance functions},
	volume = {37},
	issn = {2211-6753},
	url = {http://www.sciencedirect.com/science/article/pii/S2211675320300105},
	doi = {10.1016/j.spasta.2020.100416},
	abstract = {We focus on the discussion of modeling processes that are observed at fixed locations of a region (geostatistics). A standard approach is to assume that the process of interest follows a Gaussian Process with some mean and (valid) covariance functions. It is common to model the covariance function as the product between a variance parameter, and a correlation function which is a function of the Euclidean distance between locations. This implies that the distribution of the process is unchanged when the origin of the index set is translated, and the process is invariant under rotation about the origin; that is the process is stationary and isotropic or homogeneous. However, the assumption of stationarity and isotropy (homogeneity) rarely holds in practice. Commonly, the correlation structures of such processes are influenced by local characteristics resulting in different behaviors in neighborhoods of different spatial locations. We review models that allow for heterogeneous covariance structures and point to some avenues of future research.},
	language = {en},
	urldate = {2020-11-04},
	journal = {Spatial Statistics},
	author = {Schmidt, Alexandra M. and Guttorp, Peter},
	month = jun,
	year = {2020},
	pages = {100416},
	file = {ScienceDirect Full Text PDF:/Users/b7064522/Zotero/storage/D5EV3X6Q/Schmidt and Guttorp - 2020 - Flexible spatial covariance functions.pdf:application/pdf;ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/SPHNF6Z2/S2211675320300105.html:text/html}
}

@book{ferraty_nonparametric_2006,
	address = {New York},
	title = {Nonparametric {Functional} {Data} {Analysis}},
	isbn = {978-0-387-30369-7 978-0-387-36620-3},
	language = {English},
	publisher = {Springer Science+Business Media, Inc.},
	author = {Ferraty, Frédéric and Vieu, Philippe},
	year = {2006},
	note = {OCLC: 318296135}
}

@book{knott_interpolating_2000,
	series = {Progress in {Computer} {Science} and {Applied} {Logic}},
	title = {Interpolating {Cubic} {Splines}},
	isbn = {978-1-4612-7092-8},
	url = {https://www.springer.com/gp/book/9781461270928},
	abstract = {A spline is a thin flexible strip composed of a material such as bamboo or steel that can be bent to pass through or near given points in the plane, or in 3-space in a smooth manner. Mechanical engineers and drafting specialists find such (physical) splines useful in designing and in drawing plans for a wide variety of objects, such as for hulls of boats or for the bodies of automobiles where smooth curves need to be specified. These days, physi­ cal splines are largely replaced by computer software that can compute the desired curves (with appropriate encouragment). The same mathematical ideas used for computing "spline" curves can be extended to allow us to compute "spline" surfaces. The application ofthese mathematical ideas is rather widespread. Spline functions are central to computer graphics disciplines. Spline curves and surfaces are used in computer graphics renderings for both real and imagi­ nary objects. Computer-aided-design (CAD) systems depend on algorithms for computing spline functions, and splines are used in numerical analysis and statistics. Thus the construction of movies and computer games trav­ els side-by-side with the art of automobile design, sail construction, and architecture; and statisticians and applied mathematicians use splines as everyday computational tools, often divorced from graphic images.},
	language = {en},
	number = {1},
	urldate = {2020-11-11},
	publisher = {Birkhäuser Basel},
	author = {Knott, Gary D.},
	year = {2000},
	doi = {10.1007/978-1-4612-1320-8},
	file = {Snapshot:/Users/b7064522/Zotero/storage/7XGE9VZE/9781461270928.html:text/html}
}

@article{xiao_asymptotic_2020,
	title = {Asymptotic properties of penalized splines for functional data},
	volume = {26},
	issn = {1350-7265},
	url = {https://projecteuclid.org/euclid.bj/1598493633},
	doi = {10.3150/20-BEJ1209},
	abstract = {Penalized spline methods are popular for functional data analysis but their asymptotic properties have not been established. We present a theoretic study of the L2L2L\_\{2\} and uniform convergence of penalized splines for estimating the mean and covariance functions of functional data under general settings. The established convergence rates for the mean function estimation are mini-max rate optimal and the rates for the covariance function estimation are comparable to those using other smoothing methods.},
	language = {EN},
	number = {4},
	urldate = {2020-11-20},
	journal = {Bernoulli},
	author = {Xiao, Luo},
	month = nov,
	year = {2020},
	mrnumber = {MR4140531},
	zmnumber = {07256162},
	note = {Publisher: Bernoulli Society for Mathematical Statistics and Probability},
	pages = {2847--2875},
	file = {Snapshot:/Users/b7064522/Zotero/storage/R4PI8PWP/1598493633.html:text/html}
}

@article{wood_low-rank_2006,
	title = {Low-rank scale-invariant tensor product smooths for generalized additive mixed models},
	volume = {62},
	issn = {0006-341X},
	doi = {10.1111/j.1541-0420.2006.00574.x},
	abstract = {A general method for constructing low-rank tensor product smooths for use as components of generalized additive models or generalized additive mixed models is presented. A penalized regression approach is adopted in which tensor product smooths of several variables are constructed from smooths of each variable separately, these "marginal" smooths being represented using a low-rank basis with an associated quadratic wiggliness penalty. The smooths offer several advantages: (i) they have one wiggliness penalty per covariate and are hence invariant to linear rescaling of covariates, making them useful when there is no "natural" way to scale covariates relative to each other; (ii) they have a useful tuneable range of smoothness, unlike single-penalty tensor product smooths that are scale invariant; (iii) the relatively low rank of the smooths means that they are computationally efficient; (iv) the penalties on the smooths are easily interpretable in terms of function shape; (v) the smooths can be generated completely automatically from any marginal smoothing bases and associated quadratic penalties, giving the modeler considerable flexibility to choose the basis penalty combination most appropriate to each modeling task; and (vi) the smooths can easily be written as components of a standard linear or generalized linear mixed model, allowing them to be used as components of the rich family of such models implemented in standard software, and to take advantage of the efficient and stable computational methods that have been developed for such models. A small simulation study shows that the methods can compare favorably with recently developed smoothing spline ANOVA methods.},
	language = {eng},
	number = {4},
	journal = {Biometrics},
	author = {Wood, Simon N.},
	month = dec,
	year = {2006},
	pmid = {17156276},
	pages = {1025--1036},
	file = {Accepted Version:/Users/b7064522/Zotero/storage/UMW4ICHL/Wood - 2006 - Low-rank scale-invariant tensor product smooths fo.pdf:application/pdf}
}

@book{abramowitz_handbook_2013,
	address = {New York, NY},
	series = {Dover books on mathematics},
	title = {Handbook of mathematical functions: with formulas, graphs, and mathematical tables},
	language = {eng},
	number = {1},
	publisher = {Dover Publ},
	editor = {Abramowitz, Milton and Stegun, Irene A.},
	year = {2013}
}

@article{kay_community_2015,
	title = {The {Community} {Earth} {System} {Model} ({CESM}) large ensemble project: a community resource for studying climate change in the presence of internal climate variability},
	volume = {96},
	issn = {0003-0007},
	shorttitle = {The {Community} {Earth} {System} {Model} ({CESM}) large ensemble project},
	doi = {10.1175/BAMS-D-13-00255.1},
	language = {English},
	number = {8},
	urldate = {2021-01-15},
	journal = {Bulletin of the American Meteorological Society},
	author = {Kay, Jennifer E. and Deser, Clara and Phillips, Adam S. and Mai, A. and Hannay, Cecile and Strand, Gary and Arblaster, Julie Michelle and Bates, S. C. and Danabasoglu, Gokhan and Edwards, James C. and Holland, Marika M. and Kushner, Paul J. and Lamarque, Jean-Francois and Lawrence, David M. and Lindsay, Keith and Middleton, A. and Munoz, Ernesto and Neale, Richard B. and Oleson, Keith W. and Polvani, Lorenzo M. and Vertenstein, Mariana},
	month = aug,
	year = {2015},
	note = {Publisher: American Meteorological Society},
	pages = {1333--1349},
	file = {Snapshot:/Users/b7064522/Zotero/storage/FVMGRI83/the-community-earth-system-model-cesm-large-ensemble-project-a-co.html:text/html;Submitted Version:/Users/b7064522/Zotero/storage/ADKYLLI7/Kay et al. - 2015 - The Community Earth System Model (CESM) large ense.pdf:application/pdf}
}

@article{paciorek_spatial_2006,
	title = {Spatial {Modelling} {Using} a {New} {Class} of {Nonstationary} {Covariance} {Functions}},
	volume = {17},
	issn = {1180-4009},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2157553/},
	doi = {10.1002/env.785},
	abstract = {We introduce a new class of nonstationary covariance functions for spatial modelling. Nonstationary covariance functions allow the model to adapt to spatial surfaces whose variability changes with location. The class includes a nonstationary version of the Matérn stationary covariance, in which the differentiability of the spatial surface is controlled by a parameter, freeing one from fixing the differentiability in advance. The class allows one to knit together local covariance parameters into a valid global nonstationary covariance, regardless of how the local covariance structure is estimated. We employ this new nonstationary covariance in a fully Bayesian model in which the unknown spatial process has a Gaussian process (GP) prior distribution with a nonstationary covariance function from the class. We model the nonstationary structure in a computationally efficient way that creates nearly stationary local behavior and for which stationarity is a special case. We also suggest non-Bayesian approaches to nonstationary kriging., To assess the method, we use real climate data to compare the Bayesian nonstationary GP model with a Bayesian stationary GP model, various standard spatial smoothing approaches, and nonstationary models that can adapt to function heterogeneity. The GP models outperform the competitors, but while the nonstationary GP gives qualitatively more sensible results, it shows little advantage over the stationary GP on held-out data, illustrating the difficulty in fitting complicated spatial data.},
	number = {5},
	urldate = {2021-01-18},
	journal = {Environmetrics},
	author = {Paciorek, Christopher J. and Schervish, Mark J.},
	year = {2006},
	pmid = {18163157},
	pmcid = {PMC2157553},
	pages = {483--506},
	file = {PubMed Central Full Text PDF:/Users/b7064522/Zotero/storage/R4NNZ57L/Paciorek and Schervish - 2006 - Spatial Modelling Using a New Class of Nonstationa.pdf:application/pdf}
}

@article{lukas_robust_2006,
	title = {Robust generalized cross-validation for choosing the regularization parameter},
	volume = {22},
	issn = {0266-5611},
	url = {https://doi.org/10.1088/0266-5611/22/5/021},
	doi = {10.1088/0266-5611/22/5/021},
	abstract = {Let fλ be the regularized solution for the problem of estimating a function or vector f0 from noisy data yi = Lif0 + εi, i = 1, …, n, where Li are linear functionals. A prominent method for the selection of the crucial regularization parameter λ is generalized cross-validation (GCV). It is known that GCV has good asymptotic properties as n → ∞ but it may not be reliable for small or medium sized n, sometimes giving an estimate that is far too small. We propose a new robust GCV method (RGCV) which chooses λ to be the minimizer of γV(λ) + (1 − γ)F(λ), where V(λ) is the GCV function, F(λ) is an approximate average measure of the influence of each data point on fλ and γ ∊ (0, 1) is a robustness parameter. We show that for any n, RGCV is less likely than GCV to choose a very small value of λ, resulting in a more robust method. We also show that RGCV has good asymptotic properties as n → ∞ for general linear operator equations with uncorrelated errors. The function EF(λ) approximates the risk ER(λ) for values of λ that are asymptotically a bit smaller than the minimizer of ER(λ) (where V(λ) may not approximate well). The ‘expected’ RGCV estimate is asymptotically optimal as n → ∞ with respect to the ‘robust risk’ γER(λ) + (1 − γ)v(λ), where v(λ) is the variance component of the risk, and it has the optimal decay rate with respect to ER(λ) and stronger error criteria. The GCV and RGCV methods are compared in numerical simulations for the problem of estimating the second derivative from noisy data. The results for RGCV with n = 51 are consistent with the asymptotic results, and, for a large range of γ values, RGCV is more reliable and accurate than GCV.},
	language = {en},
	number = {5},
	urldate = {2021-01-26},
	journal = {Inverse Problems},
	author = {Lukas, Mark A.},
	month = sep,
	year = {2006},
	note = {Publisher: IOP Publishing},
	pages = {1883--1902},
	file = {Full Text:/Users/b7064522/Zotero/storage/KB6URWZK/Lukas - 2006 - Robust generalized cross-validation for choosing t.pdf:application/pdf}
}

@article{sampson_nonparametric_1992,
	title = {Nonparametric {Estimation} of {Nonstationary} {Spatial} {Covariance} {Structure}},
	volume = {87},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2290458},
	doi = {10.2307/2290458},
	abstract = {Estimation of the covariance structure of spatial processes is a fundamental prerequisite for problems of spatial interpolation and the design of monitoring networks. We introduce a nonparametric approach to global estimation of the spatial covariance structure of a random function Z(x, t) observed repeatedly at times ti (i = 1, ⋯, T) at a finite number of sampling stations xi (i = 1, 2, ..., N) in the plane. Our analyses assume temporal stationarity but do not assume spatial stationarity (or isotropy). We analyze the spatial dispersions \${\textbackslash}operatorname\{var\}(Z(x\_i, t) - Z(x\_j, t))\$ as a natural metric for the spatial covariance structure and model these as a general smooth function of the geographic coordinate of station pairs (xi, xj). The model is constructed in two steps. First, using nonmetric multidimensional scaling (MDS) we compute a two-dimensional representation of the sampling stations for which a monotone function of interpoint distances δij approximates the spatial dispersions. MDS transforms the problem into one for which the covariance structure, expressed in terms of spatial dispersions, is stationary and isotropic. Second, we compute thinplate splines to provide smooth mappings of the geographic representation of the sampling stations into their MDS representation. The composition of this mapping f and a monotone function g derived from MDS yields a nonparametric estimator of \${\textbackslash}operatorname\{var\}(Z(x\_a, t) - Z(x\_b, t))\$ for any two geographic locations xa and xb (monitored or not) of the form g(∣ f(xa) - f(xb(∣). By restricting the monotone function g to a class of conditionally nonpositive definite variogram functions, we ensure that the resulting nonparametric model corresponds to a nonnegative definite covariance model. We use biorthogonal grids, introduced by Bookstein in the field of morphometrics, to depict the thin-plate spline mappings that embody the nature of the anisotropy and nonstationarity in the sample covariance matrix. An analysis of mesoscale variability in solar radiation monitored in southwestern British Columbia demonstrates this methodology.},
	number = {417},
	urldate = {2021-01-29},
	journal = {Journal of the American Statistical Association},
	author = {Sampson, Paul D. and Guttorp, Peter},
	year = {1992},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {108--119},
	file = {JSTOR Full Text PDF:/Users/b7064522/Zotero/storage/3MX4W28Q/Sampson and Guttorp - 1992 - Nonparametric Estimation of Nonstationary Spatial .pdf:application/pdf}
}

@book{wood_generalized_2006,
	address = {Philadelphia, PA, UNITED STATES},
	title = {Generalized {Additive} {Models}: {An} {Introduction} with {R}},
	isbn = {978-1-4200-1040-4},
	shorttitle = {Generalized {Additive} {Models}},
	url = {http://ebookcentral.proquest.com/lib/ncl/detail.action?docID=1633306},
	urldate = {2021-02-26},
	publisher = {CRC Press LLC},
	author = {Wood, Simon N.},
	year = {2006},
	file = {ProQuest Ebook Snapshot:/Users/b7064522/Zotero/storage/5WFN4D4X/reader.html:text/html}
}

@inproceedings{higdon_space_2002,
	address = {London},
	title = {Space and {Space}-{Time} {Modeling} using {Process} {Convolutions}},
	isbn = {978-1-4471-0657-9},
	doi = {10.1007/978-1-4471-0657-9_2},
	abstract = {A continuous spatial model can be constructed by convolving a very simple, perhaps independent, process with a kernel or point spread function. This approach for constructing a spatial process offers a number of advantages over specification through a spatial covariogram. In particular, this process convolution specification leads to computational simplifications and easily extends beyond simple stationary models. This paper uses process convolution models to build space and space-time models that are flexible and able to accommodate large amounts of data. Data from environmental monitoring is considered.},
	language = {en},
	booktitle = {Quantitative {Methods} for {Current} {Environmental} {Issues}},
	publisher = {Springer},
	author = {Higdon, Dave},
	editor = {Anderson, Clive W. and Barnett, Vic and Chatwin, Philip C. and El-Shaarawi, Abdel H.},
	year = {2002},
	pages = {37--56}
}

@article{lindgren_explicit_2011,
	title = {An explicit link between {Gaussian} fields and {Gaussian} {Markov} random fields: the stochastic partial differential equation approach},
	volume = {73},
	copyright = {© 2011 Royal Statistical Society},
	issn = {1467-9868},
	shorttitle = {An explicit link between {Gaussian} fields and {Gaussian} {Markov} random fields},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2011.00777.x},
	doi = {https://doi.org/10.1111/j.1467-9868.2011.00777.x},
	abstract = {Summary. Continuously indexed Gaussian fields (GFs) are the most important ingredient in spatial statistical modelling and geostatistics. The specification through the covariance function gives an intuitive interpretation of the field properties. On the computational side, GFs are hampered with the big n problem, since the cost of factorizing dense matrices is cubic in the dimension. Although computational power today is at an all time high, this fact seems still to be a computational bottleneck in many applications. Along with GFs, there is the class of Gaussian Markov random fields (GMRFs) which are discretely indexed. The Markov property makes the precision matrix involved sparse, which enables the use of numerical algorithms for sparse matrices, that for fields in only use the square root of the time required by general algorithms. The specification of a GMRF is through its full conditional distributions but its marginal properties are not transparent in such a parameterization. We show that, using an approximate stochastic weak solution to (linear) stochastic partial differential equations, we can, for some GFs in the Matérn class, provide an explicit link, for any triangulation of , between GFs and GMRFs, formulated as a basis function representation. The consequence is that we can take the best from the two worlds and do the modelling by using GFs but do the computations by using GMRFs. Perhaps more importantly, our approach generalizes to other covariance functions generated by SPDEs, including oscillating and non-stationary GFs, as well as GFs on manifolds. We illustrate our approach by analysing global temperature data with a non-stationary model defined on a sphere.},
	language = {en},
	number = {4},
	urldate = {2021-04-06},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Lindgren, Finn and Rue, Håvard and Lindström, Johan},
	year = {2011},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2011.00777.x},
	pages = {423--498},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/VG95C42T/Lindgren et al. - 2011 - An explicit link between Gaussian fields and Gauss.pdf:application/pdf}
}

@article{hurrell_community_2013,
	title = {The {Community} {Earth} {System} {Model}: {A} {Framework} for {Collaborative} {Research}},
	volume = {94},
	shorttitle = {The {Community} {Earth} {System} {Model}},
	url = {https://journals.ametsoc.org/view/journals/bams/94/9/bams-d-12-00121.1.xml},
	doi = {10.1175/BAMS-D-12-00121.1},
	abstract = {{\textless}section class="abstract"{\textgreater}{\textless}p{\textgreater}The Community Earth System Model (CESM) is a flexible and extensible community tool used to investigate a diverse set of Earth system interactions across multiple time and space scales. This global coupled model significantly extends its predecessor, the Community Climate System Model, by incorporating new Earth system simulation capabilities. These comprise the ability to simulate biogeochemical cycles, including those of carbon and nitrogen, a variety of atmospheric chemistry options, the Greenland Ice Sheet, and an atmosphere that extends to the lower thermosphere. These and other new model capabilities are enabling investigations into a wide range of pressing scientific questions, providing new foresight into possible future climates and increasing our collective knowledge about the behavior and interactions of the Earth system. Simulations with numerous configurations of the CESM have been provided to phase 5 of the Coupled Model Intercomparison Project (CMIP5) and are being analyzed by the broad community of scientists. Additionally, the model source code and associated documentation are freely available to the scientific community to use for Earth system studies, making it a true community tool. This article describes this Earth system model and its various possible configurations, and highlights a number of its scientific capabilities.{\textless}/p{\textgreater}{\textless}/section{\textgreater}},
	language = {EN},
	number = {9},
	urldate = {2021-04-06},
	journal = {Bulletin of the American Meteorological Society},
	author = {Hurrell, James W. and Holland, M. M. and Gent, P. R. and Ghan, S. and Kay, Jennifer E. and Kushner, P. J. and Lamarque, J.-F. and Large, W. G. and Lawrence, D. and Lindsay, K. and Lipscomb, W. H. and Long, M. C. and Mahowald, N. and Marsh, D. R. and Neale, R. B. and Rasch, P. and Vavrus, S. and Vertenstein, M. and Bader, D. and Collins, W. D. and Hack, J. J. and Kiehl, J. and Marshall, S.},
	month = sep,
	year = {2013},
	note = {Publisher: American Meteorological Society
Section: Bulletin of the American Meteorological Society},
	pages = {1339--1360},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/KKHZ3DAW/Hurrell et al. - 2013 - The Community Earth System Model A Framework for .pdf:application/pdf}
}

@article{neale_description_nodate,
	title = {Description of the {NCAR} {Community} {Atmosphere} {Model} ({CAM} 5.0)},
	language = {en},
	author = {Neale, Richard B and Gettelman, Andrew and Park, Sungsu and Chen, Chih-Chieh and Lauritzen, Peter H and Williamson, David L and Conley, Andrew J and Kinnison, Doug and Marsh, Dan and Smith, Anne K and Vitt, Francis and Garcia, Rolando and Lamarque, Jean-Francois and Mills, Mike and Tilmes, Simone and Morrison, Hugh and Cameron-Smith, Philip and Collins, William D and Iacono, Michael J and Easter, Richard C and Liu, Xiaohong and Ghan, Steven J and Rasch, Philip J and Taylor, Mark A},
	pages = {289},
	file = {Neale et al. - Description of the NCAR Community Atmosphere Model.pdf:/Users/b7064522/Zotero/storage/CQEBU8IX/Neale et al. - Description of the NCAR Community Atmosphere Model.pdf:application/pdf}
}

@article{fan_study_1996,
	title = {A {STUDY} {OF} {VARIABLE} {BANDWIDTH} {SELECTION} {FOR} {LOCAL} {POLYNOMIAL} {REGRESSION}},
	volume = {6},
	issn = {1017-0405},
	url = {https://www.jstor.org/stable/24306002},
	abstract = {A decisive question in nonparametric smoothing techniques is the choice of the bandwidth or smoothing parameter. The present paper addresses this question when using local polynomial approximations for estimating the regression function and its derivatives. A fully-automatic bandwidth selection procedure has been proposed by Fan and Gijbels (1995a), and the empirical performance of it was tested in detail via a variety of examples. Those experiences supported the methodology towards a great extend. In this paper we establish asymptotic results for the proposed variable bandwidth selector. We provide the rate of convergence of the bandwidth estimate, and obtain the asymptotic distribution of its error relative to the theoretical optimal variable bandwidth. These asymptotic properties give extra support to the proposed bandwidth selection procedure. It is also demonstrated how the proposed selection method can be applied in the density estimation setup. some examples illustrate this application.},
	number = {1},
	urldate = {2021-04-08},
	journal = {Statistica Sinica},
	author = {Fan, Jianqing and Gijbels, Irène and Hu, Tien-Chung and Huang, Li-Shan},
	year = {1996},
	note = {Publisher: Institute of Statistical Science, Academia Sinica},
	pages = {113--127}
}

@book{bjorck_numerical_1996,
	address = {Philadelphia},
	title = {Numerical methods for least squares problems},
	isbn = {978-0-89871-360-2},
	publisher = {SIAM},
	author = {Björck, Ake},
	year = {1996}
}

@article{osullivan_statistical_1986,
	title = {A {Statistical} {Perspective} on {Ill}-{Posed} {Inverse} {Problems}},
	volume = {1},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-1/issue-4/A-Statistical-Perspective-on-Ill-Posed-Inverse-Problems/10.1214/ss/1177013525.full},
	doi = {10.1214/ss/1177013525},
	abstract = {Ill-posed inverse problems arise in many branches of science and engineering. In the typical situation one is interested in recovering a whole function given a finite number of noisy measurements on functionals. Performance characteristics of an inversion algorithm are studied via the mean square error which is decomposed into bias and variability. Variability calculations are often straightforward, but useful bias measures are more difficult to obtain. An appropriate definition of what geophysicists call the Backus-Gilbert averaging kernel leads to a natural way of measuring bias characteristics. Moreover, the ideas give rise to some important experimental design criteria. It can be shown that the optimal inversion algorithms are methods of regularization procedures, but to completely specify these algorithms the signal to noise ratio must be supplied. Statistical approaches to the empirical determination of the signal to noise ratio are discussed; cross-validation and unbiased risk methods are reviewed; and some extensions, which seem particularly appropriate in the inverse problem context, are indicated. Linear and nonlinear examples from medicine, meteorology, and geophysics are used for illustration.},
	number = {4},
	urldate = {2021-04-09},
	journal = {Statistical Science},
	author = {O'Sullivan, Finbarr},
	month = nov,
	year = {1986},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {502--518},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/UKCMETMU/O'Sullivan - 1986 - A Statistical Perspective on Ill-Posed Inverse Pro.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/BYJJXN93/1177013525.html:text/html}
}

@book{ruppert_semiparametric_2003,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {Semiparametric {Regression}},
	isbn = {978-0-521-78050-6},
	url = {https://www.cambridge.org/core/books/semiparametric-regression/02FC9A9435232CA67532B4D31874412C},
	abstract = {Semiparametric regression is concerned with the flexible incorporation of non-linear functional relationships in regression analyses. Any application area that benefits from regression analysis can also benefit from semiparametric regression. Assuming only a basic familiarity with ordinary parametric regression, this user-friendly book explains the techniques and benefits of semiparametric regression in a concise and modular fashion. The authors make liberal use of graphics and examples plus case studies taken from environmental, financial, and other applications. They include practical advice on implementation and pointers to relevant software. The 2003 book is suitable as a textbook for students with little background in regression as well as a reference book for statistically oriented scientists such as biostatisticians, econometricians, quantitative social scientists, epidemiologists, with a good working knowledge of regression and the desire to begin using more flexible semiparametric models. Even experts on semiparametric regression should find something new here.},
	urldate = {2021-04-09},
	publisher = {Cambridge University Press},
	author = {Ruppert, David and Wand, M. P. and Carroll, R. J.},
	year = {2003},
	doi = {10.1017/CBO9780511755453},
	file = {Snapshot:/Users/b7064522/Zotero/storage/YCKSQ2BR/02FC9A9435232CA67532B4D31874412C.html:text/html}
}

@article{wahba_practical_1977,
	title = {Practical {Approximate} {Solutions} to {Linear} {Operator} {Equations} when the {Data} are {Noisy}},
	volume = {14},
	issn = {0036-1429},
	url = {https://www.jstor.org/stable/2156485},
	abstract = {We consider approximate solutions fn,λ to linear operator equations Kf = g, of the form: fn,λ is the minimizer in H of (1/n)∑n j = 1 [(Kh)(tj) - y(tj)]2 + λ {\textbar} h {\textbar}2, where H is a Hilbert space, and the data \{y(tj)\} satisfy y(tj) = g(tj) + ε(tj), the \{ε(tj)\} being measurement errors. fn,λ is the so-called regularized solution, and \${\textbackslash}lambda {\textgreater} 0\$ is the regularization parameter, to be chosen. It is important to choose λ correctly. The purpose of this paper is to propose the method of weighted cross-validation for choosing λ from the data. We suppose that g is very smooth and the errors are white noise. It is shown that the weighted cross-validation estimate \${\textbackslash}hat {\textbackslash}lambda\$ estimates the value of λ which minimizes (1/n)E∑n j = 1 [(Kfn,λ)(tj) - (Kf)(tj)]2. Results related to the convergence of \${\textbackslash}{\textbar}f - f\_\{n,{\textbackslash}hat {\textbackslash}lambda\}{\textbackslash}{\textbar}\$, including rates, are obtained.},
	number = {4},
	urldate = {2021-04-12},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Wahba, Grace},
	year = {1977},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {651--667}
}

@article{wahba_comparison_1985,
	title = {A {Comparison} of {GCV} and {GML} for {Choosing} the {Smoothing} {Parameter} in the {Generalized} {Spline} {Smoothing} {Problem}},
	volume = {13},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-13/issue-4/A-Comparison-of-GCV-and-GML-for-Choosing-the-Smoothing/10.1214/aos/1176349743.full},
	doi = {10.1214/aos/1176349743},
	abstract = {The partially improper prior behind the smoothing spline model is used to obtain a generalization of the maximum likelihood (GML) estimate for the smoothing parameter. Then this estimate is compared with the generalized cross validation (GCV) estimate both analytically and by Monte Carlo methods. The comparison is based on a predictive mean square error criteria. It is shown that if the true, unknown function being estimated is smooth in a sense to be defined then the GML estimate undersmooths relative to the GCV estimate and the predictive mean square error using the GML estimate goes to zero at a slower rate than the mean square error using the GCV estimate. If the true function is "rough" then the GCV and GML estimates have asymptotically similar behavior. A Monte Carlo experiment was designed to see if the asymptotic results in the smooth case were evident in small sample sizes. Mixed results were obtained for \$n = 32\$, GCV was somewhat better than GML for \$n = 64\$, and GCV was decidedly superior for \$n = 128\$. In the \$n = 32\$ case GCV was better for smaller \${\textbackslash}sigma{\textasciicircum}2\$ and the comparison close for larger \${\textbackslash}sigma{\textasciicircum}2\$. The theoretical results are shown to extend to the generalized spline smoothing model, which includes the estimate of functions given noisy values of various integrals of them.},
	number = {4},
	urldate = {2021-04-12},
	journal = {The Annals of Statistics},
	author = {Wahba, Grace},
	month = dec,
	year = {1985},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {1378--1402},
	file = {Full Text PDF:/Users/b7064522/Zotero/storage/TI9G4RYP/Wahba - 1985 - A Comparison of GCV and GML for Choosing the Smoot.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/KH8WNSEP/1176349743.html:text/html}
}

@misc{noauthor_performance_nodate,
	title = {Performance of {Robust} {GCV} and {Modified} {GCV} for {Spline} {Smoothing} - {LUKAS} - 2012 - {Scandinavian} {Journal} of {Statistics} - {Wiley} {Online} {Library}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2011.00736.x},
	urldate = {2021-04-12},
	file = {Performance of Robust GCV and Modified GCV for Spline Smoothing - LUKAS - 2012 - Scandinavian Journal of Statistics - Wiley Online Library:/Users/b7064522/Zotero/storage/3FUBS5C2/j.1467-9469.2011.00736.html:text/html}
}

@article{lukas_performance_2012,
	title = {Performance of {Robust} {GCV} and {Modified} {GCV} for {Spline} {Smoothing}},
	volume = {39},
	copyright = {© 2011 Board of the Foundation of the Scandinavian Journal of Statistics},
	issn = {1467-9469},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2011.00736.x},
	doi = {https://doi.org/10.1111/j.1467-9469.2011.00736.x},
	abstract = {Abstract. While it is a popular selection criterion for spline smoothing, generalized cross-validation (GCV) occasionally yields severely undersmoothed estimates. Two extensions of GCV called robust GCV (RGCV) and modified GCV have been proposed as more stable criteria. Each involves a parameter that must be chosen, but the only guidance has come from simulation results. We investigate the performance of the criteria analytically. In most studies, the mean square prediction error is the only loss function considered. Here, we use both the prediction error and a stronger Sobolev norm error, which provides a better measure of the quality of the estimate. A geometric approach is used to analyse the superior small-sample stability of RGCV compared to GCV. In addition, by deriving the asymptotic inefficiency for both the prediction error and the Sobolev error, we find intervals for the parameters of RGCV and modified GCV for which the criteria have optimal performance.},
	language = {en},
	number = {1},
	urldate = {2021-04-12},
	journal = {Scandinavian Journal of Statistics},
	author = {Lukas, Mark A. and Hoog, Frank R. De and Anderssen, Robert S.},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9469.2011.00736.x},
	pages = {97--115},
	file = {Full Text:/Users/b7064522/Zotero/storage/A2XIARHI/Lukas et al. - 2012 - Performance of Robust GCV and Modified GCV for Spl.pdf:application/pdf;Snapshot:/Users/b7064522/Zotero/storage/7LPBTZD9/j.1467-9469.2011.00736.html:text/html}
}

@article{cummins_confidence_2001,
	title = {Confidence {Intervals} for {Nonparametric} {Curve} {Estimates}: {Toward} {More} {Uniform} {Pointwise} {Coverage}},
	volume = {96},
	issn = {0162-1459},
	shorttitle = {Confidence {Intervals} for {Nonparametric} {Curve} {Estimates}},
	url = {https://www.jstor.org/stable/2670362},
	abstract = {Numerous nonparametric regression methods exist that yield consistent estimators of function curves. Often, one is also interested in constructing confidence intervals for the unknown function. When a function estimate is based on a single global smoothing parameter the resulting confidence intervals may hold their desired confidence level 1 -α on average but because bias in nonparametric estimation is not uniform, they do not hold the desired level uniformly at all design points. Most research in this area has focused on mean squared error properties of the estimator, for example MISE, itself a global measure. In addition, measures like MISE are one step removed from the practical issue of coverage probability. Recent work that focuses on coverage probability has considered only coverage in an average sense, ignoring the important issue of uniformity of coverage across the design space. To deal with this problem, a new estimator is developed which uses a local cross-validation criterion (LCV) to determine a separate smoothing parameter for each design point. The local smoothing parameters are then used to compute the point estimators of the regression curve and the corresponding pointwise confidence intervals. Incorporation of local information through the new method is shown, via Monte Carlo simulation, to yield more uniformly valid pointwise confidence intervals for nonparametric regression curves. Diagnostic plots are developed (Breakout Plots) to visually inspect the degree of uniformity of coverage of the confidence intervals. The approach, here applied to cubic smoothing splines, easily generalizes to many other nonparametric regression estimators. The improved curve estimation is not a solely theoretical improvement such as providing an estimator that has a faster EASE convergence rate but shows its worth empirically by yielding improved coverage probabilities through reliable pointwise confidence intervals.},
	number = {453},
	urldate = {2021-04-12},
	journal = {Journal of the American Statistical Association},
	author = {Cummins, David J. and Filloon, Tom G. and Nychka, Douglas},
	year = {2001},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {233--246}
}

@article{aguilera_forecasting_1999,
	title = {Forecasting time series by functional {PCA}. {Discussion} of several weighted approaches},
	volume = {14},
	issn = {1613-9658},
	url = {https://doi.org/10.1007/s001800050025},
	doi = {10.1007/s001800050025},
	abstract = {In this paper a functional principal component model is applied to forecast a continuous time series that has been observed only at discrete time points not necessarily equally spaced. To take into account the natural order among the sample paths obtained after cutting the series into pieces, a weighted estimation of the principal components is proposed. In order to estimate the weighted functional principal component analysis, a cubic spline interpolation of the sample paths between their discrete observations is performed. Finally, an application with simulated data is developed where model fitting and forecasting results using different types of weightings on equally and unequally spaced data are given and discussed. The forecasting performance of the estimated functional principal component models is also compared with multivariate principal component regression models.},
	language = {en},
	number = {3},
	urldate = {2021-04-12},
	journal = {Computational Statistics},
	author = {Aguilera, Ana M. and Ocaña, Francisco A. and Valderrama, Mariano J.},
	month = sep,
	year = {1999},
	pages = {443--467},
	file = {Springer Full Text PDF:/Users/b7064522/Zotero/storage/SD5L5BWZ/Aguilera et al. - 1999 - Forecasting time series by functional PCA. Discuss.pdf:application/pdf}
}

@article{hyndman_forecasting_2009,
	title = {Forecasting functional time series},
	abstract = {We propose forecasting functional time series using weighted functional principal component regression and weighted functional partial least squares regression. These approaches allow for smooth functions, assign higher weights to more recent data, and provide a modeling scheme that is easily adapted to allow for constraints and other information. We illustrate our approaches using age-specific French female mortality rates from 1816 to 2006 and age-specific Australian fertility rates from 1921 to 2006, and show that these weighted methods improve forecast accuracy in comparison to their unweighted counterparts. We also propose two new bootstrap methods to construct prediction intervals, and evaluate and compare their empirical coverage probabilities.},
	language = {en},
	journal = {Journal of the Korean Statistical Society},
	author = {Hyndman, Rob J and Shang, Han Lin},
	year = {2009},
	pages = {13},
	file = {Hyndman and Shang - 2009 - Forecasting functional time series.pdf:/Users/b7064522/Zotero/storage/8WV3JDWS/Hyndman and Shang - 2009 - Forecasting functional time series.pdf:application/pdf}
}

@article{hyndman_stochastic_2008,
	title = {Stochastic population forecasts using functional data models for mortality, fertility and migration},
	volume = {24},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207008000319},
	doi = {10.1016/j.ijforecast.2008.02.009},
	abstract = {Age–sex-specific population forecasts are derived through stochastic population renewal using forecasts of mortality, fertility and net migration. Functional data models with time series coefficients are used to model age-specific mortality and fertility rates. As detailed migration data are lacking, net migration by age and sex is estimated as the difference between historic annual population data and successive populations one year ahead derived from a projection using fertility and mortality data. This estimate, which includes error, is also modeled using a functional data model. The three models involve different strengths of the general Box–Cox transformation chosen to minimise out-of-sample forecast error. Uncertainty is estimated from the model, with an adjustment to ensure that the one-step-forecast variances are equal to those obtained with historical data. The three models are then used in a Monte Carlo simulation of future fertility, mortality and net migration, which are combined using the cohort-component method to obtain age-specific forecasts of the population by sex. The distribution of the forecasts provides probabilistic prediction intervals. The method is demonstrated by making 20-year forecasts using Australian data for the period 1921–2004. The advantages of our method are: (1) it is a coherent stochastic model of the three demographic components; (2) it is estimated entirely from historical data with no subjective inputs required; and (3) it provides probabilistic prediction intervals for any demographic variable that is derived from population numbers and vital events, including life expectancies, total fertility rates and dependency ratios.},
	language = {en},
	number = {3},
	urldate = {2021-04-12},
	journal = {International Journal of Forecasting},
	author = {Hyndman, Rob J. and Booth, Heather},
	month = jul,
	year = {2008},
	pages = {323--342},
	file = {ScienceDirect Full Text PDF:/Users/b7064522/Zotero/storage/PPLTG9ZF/Hyndman and Booth - 2008 - Stochastic population forecasts using functional d.pdf:application/pdf;ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/7NHDZLBM/S0169207008000319.html:text/html}
}

@article{hyndman_robust_2007,
	title = {Robust forecasting of mortality and fertility rates: {A} functional data approach},
	volume = {51},
	issn = {0167-9473},
	shorttitle = {Robust forecasting of mortality and fertility rates},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947306002453},
	doi = {10.1016/j.csda.2006.07.028},
	abstract = {A new method is proposed for forecasting age-specific mortality and fertility rates observed over time. This approach allows for smooth functions of age, is robust for outlying years due to wars and epidemics, and provides a modelling framework that is easily adapted to allow for constraints and other information. Ideas from functional data analysis, nonparametric smoothing and robust statistics are combined to form a methodology that is widely applicable to any functional time series data observed discretely and possibly with error. The model is a generalization of the Lee–Carter (LC) model commonly used in mortality and fertility forecasting. The methodology is applied to French mortality data and Australian fertility data, and the forecasts obtained are shown to be superior to those from the LC method and several of its variants.},
	language = {en},
	number = {10},
	urldate = {2021-04-12},
	journal = {Computational Statistics \& Data Analysis},
	author = {Hyndman, Rob J. and Shahid Ullah, Md.},
	month = jun,
	year = {2007},
	pages = {4942--4956},
	file = {ScienceDirect Full Text PDF:/Users/b7064522/Zotero/storage/QYAK96ZW/Hyndman and Shahid Ullah - 2007 - Robust forecasting of mortality and fertility rate.pdf:application/pdf;ScienceDirect Snapshot:/Users/b7064522/Zotero/storage/TWEZPQQT/S0167947306002453.html:text/html}
}

@book{billingsley_probability_1995,
	title = {Probability and {Measure}},
	isbn = {978-0-471-00710-4},
	abstract = {PROBABILITY AND MEASURE  Third Edition  Now in its new third edition, Probability and Measure offers advanced students, scientists, and engineers an integrated introduction to measure theory and probability. Retaining the unique approach of the previous editions, this text interweaves material on probability and measure, so that probability problems generate an interest in measure theory and measure theory is then developed and applied to probability. Probability and Measure provides thorough coverage of probability, measure, integration, random variables and expected values, convergence of distributions, derivatives and conditional probability, and stochastic processes. The Third Edition features an improved treatment of Brownian motion and the replacement of queuing theory with ergodic theory.  Like the previous editions, this new edition will be well received by students of mathematics, statistics, economics, and a wide variety of disciplines that require a solid understanding of probability theory.},
	language = {en},
	publisher = {Wiley},
	author = {Billingsley, Patrick},
	month = may,
	year = {1995},
	note = {Google-Books-ID: z39jQgAACAAJ}
}
